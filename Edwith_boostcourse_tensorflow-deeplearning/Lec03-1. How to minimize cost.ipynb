{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lec03-1. How to minimize cost",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUzhcZAsINs5"
      },
      "source": [
        "# Lec 03: Linear Regression and How to minimize cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMN1xDN7Lx4o"
      },
      "source": [
        "## 핵심키워드\r\n",
        "- 선형회귀(Linear Regression)\r\n",
        "- 가설(Hypothesis)\r\n",
        "- 비용함수(Cost function)\r\n",
        "- 경사 하강법(Gradient Descent)\r\n",
        "- 볼록 함수(Convex function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX4g5ZUCIwi7"
      },
      "source": [
        "## 학습(Learning)의 목표\r\n",
        "데이터를 통해 비용(cost)을 최소화하는 W와 b((Hypothesis)를 찾는 것\r\n",
        "- Hypothesis H(x) = Wx + b\r\n",
        "- Cost cost(W, b) = MEAN of { H(x) - y }^2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLcCspCaIw2j"
      },
      "source": [
        "## 경사 하강법(Gradient Descent algorithm)\r\n",
        "- 비용함수(cost function)를 최소화하는 대표적인 방법\r\n",
        "- 변수가 둘 이상일 때도 사용 가능한 방법\r\n",
        "- 무작위의 초기값을 지정한 다음, 비용 함수에서 해당 지점의 기울기를 구해 최솟값에 가까워지도록 W와 b를 업데이트 → 업데이트 이후 비용 함수에서 해당 지점의 기울기를 구하는 과정 반복\r\n",
        "- W와 b를 업데이트 할 때 학습률(learning rate)이 클수록 급격하게 변화하고 작을수록 완만하게 변화함\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY3je_WC6I4a"
      },
      "source": [
        "## 볼록 함수(Convex function)\r\n",
        "![img](http://sanghyukchun.github.io/images/post/63-3.png)\r\n",
        "- 그림과 같이 비용함수가 볼록(convex)하지 않으면 최소값을 찾지 못하고 지역해(local minimum)에서 학습을 멈출 수 있음\r\n",
        "- 학습 시 초기값 설정이 위와 같은 현상에 영향을 미칠 수 있음"
      ]
    }
  ]
}