{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lec06-1. & Lec06-2. Softmax Regression and Classification",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUzhcZAsINs5"
      },
      "source": [
        "# Lec 06-1: Softmax Regression: 기본 개념소개\r\n",
        "# Lec 06-2: Softmax Classifier의 cost함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMN1xDN7Lx4o"
      },
      "source": [
        "## 핵심키워드(Keywords)\r\n",
        "- 다항 분류(Multinomial Classification)\r\n",
        "- 소프트맥스(Softmax)\r\n",
        "- 크로스 엔트로피(Cross-entropy)\r\n",
        "- 경사하강법(Gradient Descent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX4g5ZUCIwi7"
      },
      "source": [
        "## Logistic Regression\r\n",
        "- X → $ z = θ^TX $ (Linear function) → $ g(z) =1/(1+ \\exp^{-z}) $ (Logistic function) → $ g(z) > 0.5 $ (Decision boundary) →  $ H(x) ∈ \\{0, 1\\} $ <br><br>\r\n",
        "- We can get the **hyperplane** that separates 0 and 1 labels throught the hypothesis above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLcCspCaIw2j"
      },
      "source": [
        "## Multinomial Classification\r\n",
        "- If the number of labels is more than 2, we can repeat the process of Logistic regression. (repeat for the number of classes)\r\n",
        "- ex) Classifier for class A (A or not-A), Classifier for class B (B or not-B), etc\r\n",
        "- To simplify the task, we can use the matrix. <br><br>\r\n",
        "$$ \\begin{pmatrix} w_{A1} & w_{A2} & w_{A3} \\\\ w_{B1} & w_{B2} & w_{B3} \\\\ w_{C1} & w_{C2} & w_{C3} \\\\ \\end{pmatrix} \\cdot \\begin{pmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\\\ \\end{pmatrix} = \\begin{pmatrix} w_{A1}x_1 + w_{A2}x_2 + w_{A3}x_3 +  \\\\ w_{B1}x_1 + w_{B2}x_2 + w_{B3}x_3 +  \\\\ w_{C1}x_1 + w_{C2}x_2 + w_{C3}x_3 +  \\\\ \\end{pmatrix} = \\begin{pmatrix} \\hat{y}_A \\\\ \\hat{y}_B \\\\ \\hat{y}_C \\\\ \\end{pmatrix} \\quad (\\hat{y}_K : \\textrm{predicted value of class K}) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dov05XYqxb2O"
      },
      "source": [
        "## Softmax\r\n",
        "- Just like the Sigmoid function in logistic regression, we need some function to convert **predicted values** of each class into **predicted possibilities** of each class. That is called **Softmax** function. <br><br>\r\n",
        "- $ \\LARGE S(y_i) = \\frac{e^{y_i}}{\\sum_{j=1}^{K} e^{y_j}} $ $ \\textrm{for i = 1, ..., K} $ <br><br>\r\n",
        "- There are two important changes after applying Softmax function\r\n",
        " - the predicted value of each class is bound between 0 and 1 (possibility)\r\n",
        " - the sum of all values of each class is always 1 (total possibility)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD7_ztrWvdEI"
      },
      "source": [
        "## Cross-entropy cost function\r\n",
        "- The cross functoin of multinomial classification is similar to the repitition of binary classification's cost function <br><br>\r\n",
        "- $ D(S, L) = -\\sum_{j}^{K} \\log{S_i} \\quad \\textrm{(Error/Distance of each value)} $  <br><br>\r\n",
        "- $ L = -\\frac{1}{K}\\sum_{j}^{K} D\\{ \\; S(WX_j + b), \\; L_i \\; \\} \\quad \\textrm{(Total Loss)} $ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjL3S8FIZJI8"
      },
      "source": [
        "## Gradient Descent\r\n",
        "- The optimization method of logistic regression (Gradient Descent) can also be used in multinomial classification <br><br>\r\n",
        "- $ W := W - \\alpha \\triangle L(W)  $\r\n",
        "<!-- \\frac{\\partial}{\\partial W} -->"
      ]
    }
  ]
}