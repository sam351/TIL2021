{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_training-tokenizers",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqxrtJ9qtT6B"
      },
      "source": [
        "## Subtoken Tokenization\n",
        "- Recent works have been done on tokenization, leveraging \"subtoken\" tokenization. **Subtokens** extends the previous splitting strategy to furthermore explode a word into grammatically logicial sub-components learned from the data. <br>\n",
        "- Subtoken construction reduces the size of the vocabulary you have to carry to train a Machine Learning model. <br>\n",
        "- On the other side, as one token might be exploded into multiple subtokens, the input of your model might increase and become an issue on model with non-linear complexity over the input sequence's length. <br>\n",
        "<br>\n",
        "\n",
        "## @huggingface/tokenizers library\n",
        "- Along with the transformers library, @huggingface provides a blazing fast tokenization library able to train, tokenize and decode dozens of Gb/s of text on a common multi-core machine.\n",
        "- The library is designed to provide all the required blocks to create end-to-end tokenizers in an interchangeable way. The various components of library are:\n",
        "  - **Normalizer**: Executes all the initial transformations over the initial input string. For example when you need to lowercase some text, maybe strip it, or even apply one of the common unicode normalization process, you will add a Normalizer.\n",
        "  - **PreTokenizer**: In charge of splitting the initial input string. That's the component that decides where and how to pre-segment the origin string. The simplest example would be like we saw before, to simply split on spaces.\n",
        "  - **Model**: Handles all the sub-token discovery and generation, this part is trainable and really dependant of your input data.\n",
        "  -  **Post-Processor**: Provides advanced construction features to be compatible with some of the Transformers-based SoTA models. For instance, for BERT it would wrap the tokenized sentence around [CLS] and [SEP] tokens.\n",
        "  - **Decoder**: In charge of mapping back a tokenized input to the original string. The decoder is usually chosen according to the PreTokenizer we used previously.\n",
        "  - **Trainer**: Provides training capabilities to each model.\n",
        "<br><br>\n",
        "\n",
        "- There are multiple implementations for each of the components above:\n",
        "  - **Normalizer**: Lowercase, Unicode (NFD, NFKD, NFC, NFKC), Bert, Strip, ...\n",
        "  - **PreTokenizer**: ByteLevel, WhitespaceSplit, CharDelimiterSplit, Metaspace, ...\n",
        "  - **Model**: WordLevel, BPE, WordPiece\n",
        "  - **Post-Processor**: BertProcessor, ...\n",
        "  - **Decoder**: WordLevel, BPE, WordPiece, ...\n",
        "\n",
        "- All of these building blocks can be combined to create working tokenization pipelines. In the next section we will go over our first pipeline.\n",
        "\n",
        "- This notebook will train a **Byte-Pair Encoding (BPE)** tokenizer on a quite small input for the purpose of this notebook. We will work with **the file from Peter Norving**. This file contains around 130.000 lines of raw text that will be processed by the library to generate a working tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33otI5zRP5Sh"
      },
      "source": [
        "# check execution time for whole code\n",
        "import time\n",
        "s_time = time.time()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRsPjJwwtj8J",
        "outputId": "744d1b80-6299-4496-ea9a-d87c281016d7"
      },
      "source": [
        "# document : https://pypi.org/project/tokenizers/\n",
        "!pip install tokenizers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 25.4MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIeu1OzjwVK9",
        "outputId": "b262f3f0-11a1-4583-dd36-a82d7985d0e7"
      },
      "source": [
        "file_url = 'https://raw.githubusercontent.com/dscape/spell/master/test/resources/big.txt'\n",
        "\n",
        "# Let's download the file and save it somewhere\n",
        "from requests import get\n",
        "with open('big.txt', 'wb') as big_f:\n",
        "    response = get(file_url)\n",
        "    if response.status_code == 200:\n",
        "        big_f.write(response.content)\n",
        "        !ls\n",
        "    else:\n",
        "        print(\"Unable to get the file: {}\".format(response.reason))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "big.txt  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUFXaDvZwgGu",
        "outputId": "02c2f1d0-82cb-4941-a862-735f1c371ebe"
      },
      "source": [
        "# Everything described below can be replaced by the ByteLevelBPETokenizer class. \n",
        "\n",
        "import tokenizers\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.normalizers import Lowercase, NFKC, Sequence\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "# tokenizers : 0.10.2\n",
        "print(f'tokenizers : {tokenizers.__version__}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenizers : 0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pV3dVgAwgJJ",
        "outputId": "63cf8678-d2e5-404b-ed33-0fc4a172530d"
      },
      "source": [
        "# empty Byte-Pair Encoding model (i.e. not trained model)\n",
        "tokenizer = Tokenizer(BPE())\n",
        "\n",
        "# lower-casing & unicode-normalization\n",
        "# The sequence normalizer combines multiple normalizer\n",
        "tokenizer.normalizer = Sequence([\n",
        "    NFKC(),\n",
        "    Lowercase()\n",
        "])\n",
        "\n",
        "# pre-tokenizer to convert the input to a ByteLevel representation.\n",
        "tokenizer.pre_tokenizer = ByteLevel()\n",
        "\n",
        "# decoder to recover from a tokenized input to the original one\n",
        "tokenizer.decoder = ByteLevelDecoder()\n",
        "\n",
        "tokenizer"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tokenizers.Tokenizer at 0x5599104e5cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owVhJX0j3eON",
        "outputId": "7899552b-d72a-4708-aa73-a28f245253b5"
      },
      "source": [
        "%%time\n",
        "# initialize trainer, with details about the vocabulary\n",
        "trainer = BpeTrainer(vocab_size=25000, show_progress=True, initial_alphabet=ByteLevel.alphabet())\n",
        "tokenizer.train(files=[\"big.txt\"], trainer=trainer)\n",
        "\n",
        "print(\"Trained vocab size: {}\".format(tokenizer.get_vocab_size()))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trained vocab size: 25000\n",
            "CPU times: user 7.1 s, sys: 322 ms, total: 7.42 s\n",
            "Wall time: 4.02 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei1_Hlxp0ic7",
        "outputId": "48668eac-a58a-489f-d125-3fbbc2ea2a8c"
      },
      "source": [
        "# save the output\n",
        "tokenizer.model.save('.')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./vocab.json', './merges.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NKMdu_v36NU",
        "outputId": "4b7aef59-fcb4-4e5b-8633-9839cee24dda"
      },
      "source": [
        "# load the tokenizer model\n",
        "tokenizer.model = BPE.from_file('vocab.json', 'merges.txt')\n",
        "\n",
        "# test the tokenizer model\n",
        "sent = \"This is a simple input to be tokenized\"\n",
        "encoded = tokenizer.encode(sent)\n",
        "decoded = tokenizer.decode(encoded.ids)\n",
        "\n",
        "print(encoded)\n",
        "print(f\">>> Encoded string: {encoded.tokens}\")\n",
        "\n",
        "print(f\"\\n>>> Decoded string: {decoded}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoding(num_tokens=10, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            ">>> Encoded string: ['Ġthis', 'Ġis', 'Ġa', 'Ġsimple', 'Ġin', 'put', 'Ġto', 'Ġbe', 'Ġtoken', 'ized']\n",
            "\n",
            ">>> Decoded string:  this is a simple input to be tokenized\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-iQXKajwgLX"
      },
      "source": [
        "\n",
        "The Encoding structure exposes multiple properties which are useful when working with transformers models\n",
        "\n",
        "- **normalized_str**: The input string after normalization (lower-casing, unicode, stripping, etc.)\n",
        "- **original_str**: The input string as it was provided\n",
        "- **tokens**: The generated tokens with their string representation\n",
        "- **input_ids**: The generated tokens with their integer representation\n",
        "- **attention_mask**: If your input has been padded by the tokenizer, then this would be a vector of 1 for any non padded token and 0 for padded ones.\n",
        "- **special_token_mask**: If your input contains special tokens such as [CLS], [SEP], [MASK], [PAD], then this would be a vector with 1 in places where a special token has been added.\n",
        "- **type_ids**: If your input was made of multiple \"parts\" such as (question, context), then this would be a vector with for each token the segment it belongs to.\n",
        "- **overflowing**: If your input has been truncated into multiple subparts because of a length limit (for BERT for example the sequence length is limited to 512), this will contain all the remaining overflowing parts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndcsneAP5Cm2",
        "outputId": "a273736f-7cf6-4bcf-e82c-06d8048f9c17"
      },
      "source": [
        "# Using a pretrained tokenizer (pretrained BERT tokenizer)\n",
        "# https://huggingface.co/docs/tokenizers/python/latest/quicktour.html#using-a-pretrained-tokenizer\n",
        "\n",
        "## download vocabulary file\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
        "!ls\n",
        "\n",
        "## load vocab & generate tokenizer\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "tokenizer_bert = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\n",
        "\n",
        "print('\\n>>>', tokenizer_bert)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-07 14:34:27--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.17.163\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.17.163|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 231508 (226K) [text/plain]\n",
            "Saving to: ‘bert-base-uncased-vocab.txt’\n",
            "\n",
            "bert-base-uncased-v 100%[===================>] 226.08K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2021-05-07 14:34:27 (23.5 MB/s) - ‘bert-base-uncased-vocab.txt’ saved [231508/231508]\n",
            "\n",
            "bert-base-uncased-vocab.txt  big.txt  merges.txt  sample_data  vocab.json\n",
            "\n",
            ">>> Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=True, wordpieces_prefix=##)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QITY4jo5Tfk",
        "outputId": "8bf08da5-7073-4aae-df05-6f4b3fb305e2"
      },
      "source": [
        "sent = \"This is a simple input to be tokenized\"\n",
        "encoded = tokenizer_bert.encode(sent)\n",
        "decoded = tokenizer_bert.decode(encoded.ids)\n",
        "\n",
        "print(encoded)\n",
        "print(f\">>> Encoded string: {encoded.tokens}\")\n",
        "\n",
        "print(f\"\\n>>> Decoded string: {decoded}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoding(num_tokens=11, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            ">>> Encoded string: ['[CLS]', 'this', 'is', 'a', 'simple', 'input', 'to', 'be', 'token', '##ized', '[SEP]']\n",
            "\n",
            ">>> Decoded string: this is a simple input to be tokenized\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v32FU8kYP_HN",
        "outputId": "bbd375ad-8cb4-4080-9b2b-2b4f3c7e66b5"
      },
      "source": [
        "# check execution time for whole code\n",
        "e_time = time.time()\n",
        "time_elapsed = e_time - s_time\n",
        "print(f'Total time elapsed : {int(time_elapsed//60)} min {int(time_elapsed%60)} sec')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total time elapsed : 0 min 10 sec\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}