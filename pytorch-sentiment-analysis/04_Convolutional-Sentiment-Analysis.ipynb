{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_Convolutional-Sentiment-Analysis",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClGpy3rvL1_5",
        "outputId": "75d8f9e5-1c44-4592-f92e-356031a00ad9"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "from torchtext.legacy import data, datasets\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import spacy\n",
        "\n",
        "\n",
        "torch.__version__, torchtext.__version__, spacy.__version__, np.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1.8.0+cu101', '0.9.0', '2.2.4', '1.19.5')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I39JKY87WzB9"
      },
      "source": [
        "## Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xmBt5IAbCKq",
        "outputId": "3e595a5e-7091-40a0-e387-f88b9c67728c"
      },
      "source": [
        "%%time\n",
        "\n",
        "# set random seed for reproducibility\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# download and split dataset (train, valid, test)\n",
        "TEXT = data.Field(\n",
        "    tokenize='spacy', tokenizer_language='en_core_web_sm', batch_first=True\n",
        ")\n",
        "LABEL = data.LabelField(dtype=torch.float)\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(SEED))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\raclImdb_v1.tar.gz:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:07<00:00, 11.0MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 44s, sys: 10.5 s, total: 1min 55s\n",
            "Wall time: 2min 2s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es19mu23RL4j",
        "outputId": "7467d9e7-257c-4460-f2fb-040cf01ba228"
      },
      "source": [
        "# check the type and size of dataset\n",
        "print(f'>>> type : {type(train_data)}')\n",
        "print(f'>>> Number of training examples: {len(train_data)}')   # 17500 (35%)\n",
        "print(f'>>> Number of validation examples: {len(valid_data)}') # 7500  (15%)\n",
        "print(f'>>> Number of testing examples: {len(test_data)}')     # 25000 (50%)\n",
        "print()\n",
        "\n",
        "# check one sample data\n",
        "tmp_ex = train_data.examples[0]\n",
        "tmp_dict = vars(tmp_ex)\n",
        "print('< example data >')\n",
        "print('>>> type :', type(tmp_ex))\n",
        "for key in tmp_dict:\n",
        "  print(f'>>> {key} : {tmp_dict[key]}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> type : <class 'torchtext.legacy.data.dataset.Dataset'>\n",
            ">>> Number of training examples: 17500\n",
            ">>> Number of validation examples: 7500\n",
            ">>> Number of testing examples: 25000\n",
            "\n",
            "< example data >\n",
            ">>> type : <class 'torchtext.legacy.data.example.Example'>\n",
            ">>> text : ['This', 'film', 'makes', '\"', 'American', 'Pie', '\"', 'a', 'sophisticated', 'movie', '!', 'No', 'further', 'comment', 'needed', '.', 'Humor', 'is', 'cheap', ',', 'dialogues', 'are', 'stupid', 'and', 'the', 'cast', 'is', 'awkward', '.', 'Every', 'cliché', 'is', 'used', 'several', 'times', 'without', 'any', 'original', 'twist', '.', 'And', 'far', 'the', 'worst', ',', 'the', 'movie', 'turns', 'out', 'to', 'be', 'more', 'catholic', 'than', 'the', 'Pope', '.', 'It', \"'s\", 'so', 'sad', '.']\n",
            ">>> label : neg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVOnakpiP1T-",
        "outputId": "fd11c9af-6d14-4349-8109-7bf701ca3914"
      },
      "source": [
        "%%time\n",
        "\n",
        "# build vocabulary\n",
        "MAX_VOCAB_SIZE = 25_000\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print('\\n')\n",
        "print(f\">>> Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\">>> Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
        "print(f\">>> Top 20 common tokens :{TEXT.vocab.freqs.most_common(20)}\")\n",
        "print()\n",
        "print('<itos and stoi>')\n",
        "print('>>> itos :', TEXT.vocab.itos[:10])\n",
        "print('>>> stoi :', LABEL.vocab.stoi)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.37MB/s]                           \n",
            "100%|█████████▉| 398552/400000 [00:17<00:00, 22483.06it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            ">>> Unique tokens in TEXT vocabulary: 25002\n",
            ">>> Unique tokens in LABEL vocabulary: 2\n",
            ">>> Top 20 common tokens :[('the', 202042), (',', 190852), ('.', 165459), ('and', 109109), ('a', 108897), ('of', 100136), ('to', 93264), ('is', 76237), ('in', 60973), ('I', 54024), ('it', 53056), ('that', 49066), ('\"', 43872), (\"'s\", 42879), ('this', 42306), ('-', 36575), ('/><br', 35431), ('was', 34903), ('as', 30307), ('with', 29885)]\n",
            "\n",
            "<itos and stoi>\n",
            ">>> itos : ['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n",
            ">>> stoi : defaultdict(None, {'neg': 0, 'pos': 1})\n",
            "CPU times: user 42.4 s, sys: 7.32 s, total: 49.7 s\n",
            "Wall time: 3min 25s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "vB58J1OUQZXr",
        "outputId": "6e88b11f-9735-4670-d978-a6fd760c5a10"
      },
      "source": [
        "# create the iterators\n",
        "BATCH_SIZE = 64\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device\n",
        ")\n",
        "\n",
        "display(device, type(train_iterator), len(train_iterator), len(train_data)/BATCH_SIZE)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "torchtext.legacy.data.iterator.BucketIterator"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "274"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "273.4375"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U0Ulm3zW3hX"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDyjWvlThyW-"
      },
      "source": [
        "# CNN model for fixed convolutional layers (3, 4, 5)\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, pad_idx, n_filters, filter_sizes, output_dim, dropout):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, pad_idx)\n",
        "    self.conv_0 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[0], embedding_dim))\n",
        "    self.conv_1 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[1], embedding_dim))\n",
        "    self.conv_2 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[2], embedding_dim))\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)\n",
        "  \n",
        "  def forward(self, text_ts):\n",
        "    # text_ts = [batch size, sent len]\n",
        "\n",
        "    embedded = self.embedding(text_ts)\n",
        "    embedded = embedded.unsqueeze(1)\n",
        "    # embedded = [batch size, 1, sent len, emb dim]\n",
        "\n",
        "    conv0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
        "    conv1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
        "    conv2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
        "    # conv = [batch size, n_filters, sent len-filter_sizes[n]+1]\n",
        "\n",
        "    pool0 = F.max_pool1d(conv0, conv0.shape[2]).squeeze(2)\n",
        "    pool1 = F.max_pool1d(conv1, conv1.shape[2]).squeeze(2)\n",
        "    pool2 = F.max_pool1d(conv2, conv2.shape[2]).squeeze(2)\n",
        "    # pool = [batch size, n_filters]\n",
        "\n",
        "    cat = self.dropout(torch.cat((pool0, pool1, pool2), dim=1))\n",
        "    # cat = [batch size, n_filters*len(filter_sizes)]\n",
        "\n",
        "    out = self.fc(cat)\n",
        "    # out = [batch size, output dim]\n",
        "    \n",
        "    return out"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOH7cgxzqDDR",
        "outputId": "61fb7e6f-bb4b-465a-b0ca-fd1b1221125d"
      },
      "source": [
        "# function for model summary\n",
        "def model_summary(model):\n",
        "  num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f'>>> The model has {num_parameters:,} trainable parameters')\n",
        "  print(model)\n",
        "\n",
        "\n",
        "# generate model & summary\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100  # 50-250\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]  # '<pad>' -> 1\n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [3, 4, 5]\n",
        "OUTPUT_DIM = 1  # No of labels\n",
        "DROPOUT = 0.5\n",
        "\n",
        "model = CNN(vocab_size=INPUT_DIM, embedding_dim=EMBEDDING_DIM, pad_idx=PAD_IDX, n_filters=N_FILTERS,\n",
        "            filter_sizes=FILTER_SIZES, output_dim=OUTPUT_DIM, dropout=DROPOUT).to(device)\n",
        "model_summary(model)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> The model has 2,620,801 trainable parameters\n",
            "CNN(\n",
            "  (embedding): Embedding(25002, 100, padding_idx=1)\n",
            "  (conv_0): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
            "  (conv_1): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
            "  (conv_2): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imsBloKKqDDX",
        "outputId": "6fa66a54-dea4-488c-d0b8-816df1ff3852"
      },
      "source": [
        "# test tensor shape in each step\n",
        "ts = torch.randint(low=1, high=250, size=(2, 12))\n",
        "print('>>> text_ts =', ts.shape)\n",
        "ts = model.embedding(ts).unsqueeze(1)\n",
        "print('>>> embedded =', ts.shape)\n",
        "ts = model.conv_0(ts).squeeze(-1)\n",
        "print('>>> conved[0] =', ts.shape)\n",
        "ts = F.avg_pool1d(ts, ts.shape[-1]).squeeze(-1)\n",
        "print('>>> pooled[0] =', ts.shape)\n",
        "ts = torch.cat([ts]*3, dim=1)\n",
        "print('>>> cat =', ts.shape)\n",
        "ts = model.fc(ts)\n",
        "print('>>> out =', ts.shape)\n",
        "ts.sigmoid()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> text_ts = torch.Size([2, 12])\n",
            ">>> embedded = torch.Size([2, 1, 12, 100])\n",
            ">>> conved[0] = torch.Size([2, 100, 10])\n",
            ">>> pooled[0] = torch.Size([2, 100])\n",
            ">>> cat = torch.Size([2, 300])\n",
            ">>> out = torch.Size([2, 1])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4858],\n",
              "        [0.4593]], grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWWFdHzChySQ"
      },
      "source": [
        "# CNN model for flexible convolutional layers\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, pad_idx, n_filters, filter_sizes, output_dim, dropout):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, pad_idx)\n",
        "    self.convs = nn.ModuleList([nn.Conv2d(1, n_filters, (fsize, embedding_dim)) for fsize in filter_sizes])\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.fc = nn.Linear(n_filters*len(filter_sizes), output_dim)\n",
        "  \n",
        "  def forward(self, text_ts):\n",
        "    # text_ts = [batch size, sent len]\n",
        "    \n",
        "    embedded = self.embedding(text_ts).unsqueeze(1)\n",
        "    # [batch size, 1, sent len, emb dim]\n",
        "\n",
        "    conved = [F.relu(conv(embedded)).squeeze(-1) for conv in self.convs]\n",
        "    # conved = [batch size, n_filters, sent len - filter size + 1]\n",
        "\n",
        "    pooled = [F.max_pool1d(conv, conv.shape[-1]).squeeze(-1) for conv in conved]\n",
        "    # pooled = [batch size, n_filters]\n",
        "\n",
        "    cat = self.dropout(torch.cat(pooled, dim=1))\n",
        "    # cat = [batch size, n_filters * len(filter_sizes)]\n",
        "\n",
        "    out = self.fc(cat)\n",
        "    # out = [batch size, 1]\n",
        "\n",
        "    return out"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYqx53PV2mx9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "639ac28d-4d81-4289-c2a6-779b74c2664d"
      },
      "source": [
        "# generate model & summary\n",
        "model = CNN(vocab_size=INPUT_DIM, embedding_dim=EMBEDDING_DIM, pad_idx=PAD_IDX, n_filters=N_FILTERS,\n",
        "            filter_sizes=FILTER_SIZES, output_dim=OUTPUT_DIM, dropout=DROPOUT).to(device)\n",
        "model_summary(model)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> The model has 2,620,801 trainable parameters\n",
            "CNN(\n",
            "  (embedding): Embedding(25002, 100, padding_idx=1)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
            "    (1): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
            "    (2): Conv2d(1, 100, kernel_size=(5, 100), stride=(1, 1))\n",
            "  )\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrKpr7m8lh4A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2011c1b0-b886-42fc-8b86-0af6a815f93d"
      },
      "source": [
        "# test tensor shape in each step\n",
        "ts = torch.randint(low=1, high=250, size=(2, 12))\n",
        "print('>>> text_ts =', ts.shape)\n",
        "ts = model.embedding(ts).unsqueeze(1)\n",
        "print('>>> embedded =', ts.shape)\n",
        "ts = model.convs[0](ts).squeeze(-1)\n",
        "print('>>> conved[0] =', ts.shape)\n",
        "ts = F.avg_pool1d(ts, ts.shape[-1]).squeeze(-1)\n",
        "print('>>> pooled[0] =', ts.shape)\n",
        "ts = torch.cat([ts]*3, dim=1)\n",
        "print('>>> cat =', ts.shape)\n",
        "ts = model.fc(ts)\n",
        "print('>>> out =', ts.shape)\n",
        "ts.sigmoid()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> text_ts = torch.Size([2, 12])\n",
            ">>> embedded = torch.Size([2, 1, 12, 100])\n",
            ">>> conved[0] = torch.Size([2, 100, 10])\n",
            ">>> pooled[0] = torch.Size([2, 100])\n",
            ">>> cat = torch.Size([2, 300])\n",
            ">>> out = torch.Size([2, 1])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5362],\n",
              "        [0.4801]], grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA-hnGUOW4C8"
      },
      "source": [
        "# 1-Dimensional CNN model for flexible convolutional layers\n",
        "class CNN1d(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, pad_idx, output_dim, n_filters, filter_sizes, dropout):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, pad_idx)\n",
        "    self.convs = nn.ModuleList([nn.Conv1d(in_channels=embedding_dim, out_channels=n_filters, kernel_size=fs)\n",
        "                                for fs in filter_sizes])\n",
        "    self.droptout = nn.Dropout(dropout)\n",
        "    self.fc = nn.Linear(n_filters * len(filter_sizes), output_dim)\n",
        "\n",
        "  def forward(self, text_ts):\n",
        "    # text_ts = [batch size, sent len]\n",
        "\n",
        "    embedded = self.embedding(text_ts).permute(0, 2, 1)\n",
        "    # embedded = [batch size, emb dim, sent len]\n",
        "\n",
        "    conved = [conv(embedded) for conv in self.convs]\n",
        "    # conved = [batch size, n_filters, sent len - filter_sizes[0] + 1]\n",
        "\n",
        "    pooled = [F.max_pool1d(c, c.shape[-1]).squeeze(-1) for c in conved]\n",
        "    # pooled = [batch size, n_filters]\n",
        "\n",
        "    cat = self.droptout(torch.cat(pooled, dim=1))\n",
        "    # cat = [batch size, n_filters * len(filter_sizes)]\n",
        "\n",
        "    out = self.fc(cat)\n",
        "    # out = [batch size, 1]\n",
        "\n",
        "    return out"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47gb-Q3zFKgt",
        "outputId": "4c624052-c33b-4700-e570-9e52978bd808"
      },
      "source": [
        "# generate model & summary\n",
        "model = CNN1d(vocab_size=INPUT_DIM, embedding_dim=EMBEDDING_DIM, pad_idx=PAD_IDX, n_filters=N_FILTERS,\n",
        "            filter_sizes=FILTER_SIZES, output_dim=OUTPUT_DIM, dropout=DROPOUT).to(device)\n",
        "model_summary(model)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> The model has 2,620,801 trainable parameters\n",
            "CNN1d(\n",
            "  (embedding): Embedding(25002, 100, padding_idx=1)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
            "    (1): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
            "    (2): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (droptout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=300, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrxY79uFH6jd",
        "outputId": "39a1241d-46b5-4bc2-a6e1-0b11cb6ad36c"
      },
      "source": [
        "# test tensor shape in each step\n",
        "ts = torch.randint(low=1, high=250, size=(2, 12))\n",
        "print('>>> text_ts =', ts.shape)\n",
        "ts = model.embedding(ts).permute(0, 2, 1)\n",
        "print('>>> embedded =', ts.shape)\n",
        "ts = model.convs[0](ts)\n",
        "print('>>> conved[0] =', ts.shape)\n",
        "ts = F.avg_pool1d(ts, ts.shape[-1]).squeeze(-1)\n",
        "print('>>> pooled[0] =', ts.shape)\n",
        "ts = torch.cat([ts]*3, dim=1)\n",
        "print('>>> cat =', ts.shape)\n",
        "ts = model.fc(ts)\n",
        "print('>>> out =', ts.shape)\n",
        "ts.sigmoid()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> text_ts = torch.Size([2, 12])\n",
            ">>> embedded = torch.Size([2, 100, 12])\n",
            ">>> conved[0] = torch.Size([2, 100, 10])\n",
            ">>> pooled[0] = torch.Size([2, 100])\n",
            ">>> cat = torch.Size([2, 300])\n",
            ">>> out = torch.Size([2, 1])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4454],\n",
              "        [0.5458]], grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "docsyRD_FKmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55f40220-6b9d-4d17-e35f-75f9f0bab35b"
      },
      "source": [
        "# generate final model\n",
        "model = CNN(vocab_size=INPUT_DIM, embedding_dim=EMBEDDING_DIM, pad_idx=PAD_IDX, n_filters=N_FILTERS,\n",
        "            filter_sizes=FILTER_SIZES, output_dim=OUTPUT_DIM, dropout=DROPOUT).to(device)\n",
        "\n",
        "\n",
        "# check original weights\n",
        "original_weights = model.embedding.weight.data\n",
        "print('>>> original initial weights :\\n', original_weights.shape)\n",
        "print(original_weights)\n",
        "\n",
        "# replace initial weights of embedding layer with pre-trained embeddings\n",
        "model.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
        "\n",
        "# replace initial weights of unk & pad tokens with zeros\n",
        "UNK_IDX = TEXT.vocab.unk_index  # '<unk>' -> 0\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "# check replaced weights\n",
        "print('\\n>>> replaced initial weights :\\n', model.embedding.weight.data.shape)\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> original initial weights :\n",
            " torch.Size([25002, 100])\n",
            "tensor([[ 1.2770, -0.7649,  1.1277,  ...,  1.0343,  0.8680, -0.1373],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 1.1673,  0.9444, -0.4215,  ..., -0.5522,  2.1897,  0.6917],\n",
            "        ...,\n",
            "        [ 0.0637,  0.4960, -0.0610,  ...,  0.9848,  0.6700, -0.0919],\n",
            "        [-1.0118,  0.7168,  2.0885,  ...,  1.3343, -0.9327,  0.8813],\n",
            "        [ 0.4971, -0.7452,  0.7343,  ..., -2.3733,  0.1873, -0.0789]])\n",
            "\n",
            ">>> replaced initial weights :\n",
            " torch.Size([25002, 100])\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "        ...,\n",
            "        [ 0.2817,  0.8455,  0.5253,  ..., -0.1279, -0.5278,  0.0667],\n",
            "        [ 0.0136, -0.1332,  0.1959,  ..., -0.1742, -0.2533,  1.0006],\n",
            "        [-0.2638,  1.1679, -0.3677,  ...,  0.2743, -1.2740, -0.5470]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ5N8590p9iU"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}