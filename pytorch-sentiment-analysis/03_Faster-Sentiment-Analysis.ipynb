{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_Faster-Sentiment-Analysis",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClGpy3rvL1_5",
        "outputId": "405681d7-f07d-4440-ab50-206cf373d9fb"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "import torchtext\r\n",
        "from torchtext.legacy import data, datasets\r\n",
        "\r\n",
        "import random\r\n",
        "import time\r\n",
        "\r\n",
        "import spacy\r\n",
        "\r\n",
        "torch.__version__, torchtext.__version__, spacy.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1.8.0+cu101', '0.9.0', '2.2.4')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I39JKY87WzB9"
      },
      "source": [
        "## Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xmBt5IAbCKq",
        "outputId": "44e75724-ea49-4217-c83e-850d2a89ff38"
      },
      "source": [
        "%%time\r\n",
        "\r\n",
        "# function for adding ngrams to token list\r\n",
        "def append_ngrams(x:list, N=2):\r\n",
        "  ngrams = set(zip(*[x[i:] for i in range(N)]))\r\n",
        "  for ngram in ngrams:\r\n",
        "    x.append(' '.join(ngram))\r\n",
        "  return x\r\n",
        "\r\n",
        "# set random seed for reproducibility\r\n",
        "SEED = 1234\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True\r\n",
        "\r\n",
        "# download and split dataset (train, valid, test)\r\n",
        "TEXT = data.Field(\r\n",
        "    preprocessing=append_ngrams, tokenize='spacy', tokenizer_language='en_core_web_sm'\r\n",
        ")\r\n",
        "LABEL = data.LabelField(dtype=torch.float)\r\n",
        "\r\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\r\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(SEED))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 6s, sys: 11.8 s, total: 2min 18s\n",
            "Wall time: 2min 56s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es19mu23RL4j",
        "outputId": "57835048-a068-40d9-da0a-254a1afc9eab"
      },
      "source": [
        "# check the type and size of dataset\r\n",
        "print(f'>>> type : {type(train_data)}')\r\n",
        "print(f'>>> Number of training examples: {len(train_data)}')   # 17500 (35%)\r\n",
        "print(f'>>> Number of validation examples: {len(valid_data)}') # 7500  (15%)\r\n",
        "print(f'>>> Number of testing examples: {len(test_data)}')     # 25000 (50%)\r\n",
        "print()\r\n",
        "\r\n",
        "# check one sample data\r\n",
        "tmp_ex = train_data.examples[0]\r\n",
        "tmp_dict = vars(tmp_ex)\r\n",
        "print('< example data >')\r\n",
        "print('>>> type :', type(tmp_ex))\r\n",
        "for key in tmp_dict:\r\n",
        "  print(f'>>> {key} : {tmp_dict[key]}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> type : <class 'torchtext.legacy.data.dataset.Dataset'>\n",
            ">>> Number of training examples: 17500\n",
            ">>> Number of validation examples: 7500\n",
            ">>> Number of testing examples: 25000\n",
            "\n",
            "< example data >\n",
            ">>> type : <class 'torchtext.legacy.data.example.Example'>\n",
            ">>> text : ['[', 'CONTAINS', 'SPOILERS!!!]<br', '/><br', '/', '>', 'Timon', 'and', 'Pumbaa', 'are', 'watching', 'The', 'Lion', 'King', '.', 'Timon', 'decides', 'to', 'go', 'back', 'BEFORE', 'the', 'beginning', ',', 'to', 'when', 'the', 'story', 'really', 'began', '.', 'So', 'they', 'go', 'back', '.', 'Way', 'back', '.', 'Back', 'even', 'before', 'Simba', 'was', 'born', '.', 'Back', 'to', 'Timon', \"'s\", 'old', 'home', 'which', 'was', 'miles', 'away', 'from', 'Pride', 'Rock', '.', 'A', 'clan', 'of', 'meerkats', 'burrowed', 'underground', 'to', 'hide', 'from', 'hyenas', '.', 'The', 'worst', 'digger', 'in', 'the', 'clan', 'was', 'a', 'pompous', ',', 'self', '-', 'centered', 'meerkat', 'named', 'Timon', '.', 'His', 'mother', 'took', 'pity', 'on', 'him', 'but', 'Uncle', 'Max', 'just', 'shook', 'his', 'head', '.', 'Mother', 'suggested', 'putting', 'Timon', 'on', 'sentry', 'duty', ';', 'Timon', 'had', 'dreams', 'of', 'a', 'bigger', 'and', 'better', 'place', 'out', 'there', 'somewhere', '.', 'Just', 'then', ',', 'hyenas', 'Shenzi', ',', 'Bonzai', 'and', 'Ed', 'arrived', 'and', 'nearly', 'killed', 'poor', 'Uncle', 'Max', '.', 'That', 'did', 'it', '.', 'The', 'other', 'meerkats', 'just', 'wanted', 'Timon', 'to', 'go', 'away', 'while', 'Timon', 'took', 'it', 'upon', 'himself', 'to', 'leave', '.', 'So', 'he', 'kissed', 'his', 'mom', 'goodbye', 'and', 'started', 'off', '.', 'He', 'did', \"n't\", 'get', 'very', 'far', 'before', 'he', 'started', 'getting', 'homesick', '.', 'Just', 'then', 'he', 'met', 'Rafiki', ',', 'who', 'taught', 'him', 'to', 'look', 'beyond', 'what', 'he', 'sees', '.', 'Timon', 'had', 'no', 'clue', 'what', 'that', 'meant', 'so', 'he', 'continued', 'on', 'and', 'met', 'a', 'warthog', 'named', 'Pumbaa', ',', 'who', 'was', 'all', 'alone', 'due', 'to', 'a', 'flatulence', 'problem', '.', 'Timon', 'and', 'Pumbaa', 'join', 'up', 'then', ',', 'but', 'Timon', 'declared', 'them', 'acquaintances', ',', 'rather', 'than', 'friends.<br', '/><br', '/', '>', 'They', 'soon', 'arrive', 'at', 'Pride', 'Rock', 'where', 'all', 'the', 'zebras', ',', 'antelopes', ',', 'wildebeests', ',', 'rhinoceroses', ',', 'giraffe', \"'s\", ',', 'elephants', 'and', 'many', 'other', 'plain', 'animals', 'had', 'gathered', '.', 'What', 'was', 'going', 'on', '?', 'Timon', 'did', \"n't\", 'care', '.', 'They', 'pressed', 'on', '.', 'Timon', 'then', 'saw', 'Rafiki', 'atop', 'Pride', 'Rock', 'lifting', 'into', 'the', 'air', 'something', 'he', 'could', \"n't\", 'see', '.', 'Just', 'then', 'all', 'the', 'animals', 'took', 'a', 'bow', '.', 'Was', 'this', 'to', 'honor', 'the', 'birth', 'of', 'the', 'new', 'king', '?', 'No', ',', 'Pumbaa', 'had', 'passed', 'gas', 'and', 'the', 'animals', 'were', 'bowing', 'to', 'cover', 'their', 'noses', ';', 'Timon', 'and', 'Pumbaa', 'try', 'an', 'assortment', 'of', 'new', 'homes', ',', 'but', 'each', 'are', 'discomforting', 'due', 'to', 'incessant', 'singing', 'or', 'hyenas', 'or', 'a', 'large', 'stampede', 'of', 'wildebeests', '!', 'Pumbaa', 'and', 'Timon', 'suddenly', 'find', 'themselves', 'heading', 'down', 'stream', '.', 'When', 'they', 'reach', 'land', ',', 'Timon', 'decides', 'to', 'give', 'up', '.', 'But', 'then', 'they', 'gaze', 'around', 'at', 'their', 'newfound', 'paradise', '.', 'It', 'was', 'beautiful', ':', 'trees', 'and', 'water', 'falls', 'as', 'far', 'as', 'the', 'eye', 'could', 'see', '.', 'Timon', 'named', 'the', 'place', 'after', 'a', 'strange', 'phrase', 'he', 'learned', 'from', 'Rafiki', ':', 'Hakuna', 'Matata', '.', 'Timon', 'and', 'Pumbaa', 'go', 'out', 'bowling', 'for', 'buzzards', 'one', 'afternoon', 'when', 'they', 'suddenly', 'run', 'into', 'Simba', '.', 'They', 'take', 'him', 'under', 'their', 'wing', 'and', 'become', 'father', 'figures', '.', 'They', 'teach', 'him', 'the', 'arts', 'of', 'bug', 'eating', 'and', 'belching', 'contests', '.', 'Pretty', 'soon', ',', 'a', 'teenage', 'Simba', 'takes', 'on', 'Timon', 'in', 'a', 'snail', 'slurping', 'contest', '.', 'Simba', 'won', ',', 'leaving', 'Timon', 'deathly', 'ill.<br', '/><br', '/', '>', 'Then', 'one', 'day', ',', 'Simba', \"'s\", 'childhood', 'friend', 'Nala', 'arrived', '.', 'Timon', 'and', 'Pumbaa', 'just', 'knew', 'she', \"'d\", 'break', 'up', 'the', 'friendship', '.', 'Suddenly', ',', 'Simba', 'runs', 'away', '.', 'Nala', 'and', 'Pumbaa', 'race', 'after', 'him', ',', 'but', 'not', 'Timon', '.', 'He', 'chose', 'to', 'stay', 'at', '\"', 'Hakuna', 'Matata', '\"', 'by', 'himself', ',', 'until', 'Rafiki', '\"', 'talked', '\"', 'some', 'sense', 'into', 'him', ',', 'so', 'he', 'joins', 'his', 'friends', 'at', 'Pride', 'Rock', '.', 'Timon', \"'s\", 'mother', 'and', 'Uncle', 'Max', 'arrive', 'then', '.', 'While', 'Simba', 'battles', 'Scar', ',', 'Mother', 'and', 'Max', 'dig', 'a', 'large', 'hole', 'to', 'trap', 'hyenas', 'Shenzi', ',', 'Bonzai', 'and', 'Ed', 'in', '.', 'It', 'worked', '.', 'Scar', 'is', 'soon', 'flung', 'down', 'the', 'same', 'hole', 'where', 'he', 'is', 'devoured', 'by', 'the', 'hyenas', '.', 'Then', 'all', 'is', 'well', '.', 'Mother', ',', 'Uncle', 'Max', 'and', 'the', 'rest', 'of', 'the', 'meerkats', 'go', 'live', 'with', 'Timon', 'and', 'Pumbaa', 'in', 'the', 'paradise', 'that', 'is', 'Hakuna', 'Matata', '.', 'Back', 'to', 'the', 'present', ',', 'Timon', 'and', 'Pumbaa', 'finish', 'the', 'movie', 'when', 'suddenly', 'Mother', ',', 'Uncle', 'Max', ',', 'Simba', 'and', 'Rafiki', 'want', 'to', 'watch', 'it', 'again', '.', 'So', 'do', 'Mickey', 'Mouse', ',', 'Donald', 'Duck', ',', 'Goofy', ',', 'Snow', 'White', ',', 'the', 'Seven', 'Dwarfs', ',', 'Dumbo', ',', 'Peter', 'Pan', ',', 'the', 'Lost', 'Boys', ',', 'Mad', 'Hatter', ',', 'March', 'Hare', ',', 'Genie', ',', 'Aladdin', ',', 'and', 'Jasmine.<br', '/><br', '/', '>', 'Well', ',', 'I', 'must', 'say', 'that', 'The', 'Lion', 'King', '1', '1/2', 'was', \"n't\", 'as', 'good', 'as', 'I', 'had', 'hoped', '.', 'It', 'was', 'too', 'ridiculous', 'and', 'silly', '.', 'The', 'original', 'Lion', 'King', 'was', 'a', 'masterpiece', '.', 'It', 'had', 'a', 'serious', 'story', 'with', 'light', 'comedy', 'thrown', 'in', '.', 'This', 'one', 'was', 'just', 'silly', 'and', 'made', 'a', 'mockery', 'of', 'it', '.', 'I', 'swear', ',', 'sometimes', 'Timon', 'and', 'Pumbaa', 'are', 'just', 'way', 'too', 'overplayed', '.', 'They', \"'re\", 'overplayed', 'to', 'the', 'point', 'of', 'no', 'longer', 'being', 'funny', ',', 'just', 'annoying', '.', 'The', 'original', 'voice', 'cast', 'is', 'back', ':', 'Nathan', 'Lane', 'as', 'Timon', ',', 'Ernie', 'Sabella', 'as', 'Pumbaa', ',', 'Matthew', 'Broddrick', 'as', 'Adult', 'Simba', ',', 'Whoopi', 'Goldberg', 'as', 'Shenzi', ',', 'Cheech', 'Marin', 'as', 'Bonzai', ',', 'Jim', 'Cummings', 'as', 'Ed', ',', 'Robert', 'Guillame', 'as', 'Rafiki', '.', 'New', 'to', 'the', 'cast', 'are', 'Julie', 'Kavner', 'of', 'TV', \"'s\", '(', 'Too', ')', 'long', 'running', 'series', 'The', 'Simpsons', 'as', 'Timon', \"'s\", 'mom', 'and', 'Jerry', 'Stiller', 'as', 'Uncle', 'Max', '.', 'So', 'anyway', ',', 'this', 'movie', 'is', \"n't\", 'The', 'Lion', 'King', 'III', ',', 'and', 'it', 'is', \"n't\", 'II', 'because', 'there', 'already', 'is', 'a', 'II', '.', 'It', 'takes', 'place', 'right', 'after', 'Part', 'I', 'and', 'Part', 'II', 'is', 'a', 'ways', 'away', '.', 'Hence', ',', 'it', \"'s\", '1', '1/2', '.', 'In', 'conclusion', ',', 'I', 'do', \"n't\", 'recommend', 'this', 'to', 'die', 'hard', 'Lion', 'King', 'fans', 'because', 'it', \"'s\", 'far', 'too', 'ridiculous', 'and', 'frivilous', '.', 'However', 'the', 'kids', 'will', 'love', 'it', 'so', 'I', 'recommend', 'it', 'to', 'them', '.', 'I', 'hope', 'this', 'will', 'also', 'be', 'the', 'LAST', 'Lion', 'King', 'movie', '.', 'Two', 'is', 'enough', '.', '\"', 'The', 'Lion', 'King', '1', '1/2', '\"', '.', 'What', 'we', \"'ve\", 'come', 'to', 'expect', 'from', 'Disney', 'sequel', 'makers.<br', '/><br', '/>-', 'it is', 'friends at', 'Back even', 'this will', 'just knew', 'White ,', 'then he', 'he sees', 'on Timon', 'rather than', 'back :', 'them .', '. Simba', 'discomforting due', 'the meerkats', '. That', ', Genie', 'The other', 'to Timon', '. Mother', 'an assortment', 'take him', 'it so', 'Timon had', 'back BEFORE', \"n't recommend\", '\" The', 'snail slurping', ', self', 'talked \"', 'all alone', ', giraffe', 'he joins', 'had a', 'Just then', 'are discomforting', 'but Uncle', 'met a', 'Nala arrived', 'poor Uncle', 'large hole', 'is a', \"n't get\", 'far before', 'he is', 'When they', 'Ed ,', 'and frivilous', \"giraffe 's\", 'he could', 'they reach', 'Simba takes', 'even before', 'underground to', 'on .', 'and Jasmine.<br', 'a masterpiece', ', who', \"n't The\", 'In conclusion', ', a', \"'s far\", 'beginning ,', 'Rock where', 'away while', 'off .', 'when the', 'continued on', ', Mad', 'many other', 'a teenage', 'takes place', 'will also', \"'d break\", 'pompous ,', 'named Timon', 'hide from', 'while Timon', 'the LAST', 'each are', 'had gathered', 'by himself', 'bigger and', 'Pumbaa race', 'himself to', 'birth of', 'there already', 'singing or', 'honor the', 'with light', 'into Simba', 'leaving Timon', 'took pity', 'is enough', '! Pumbaa', \"did n't\", 'he met', 'one day', 'clan of', '- centered', 'wildebeests !', 'It was', 'and the', 'declared them', ', Uncle', 'run into', \"'ve come\", 'Back to', '> Then', ', to', 'rhinoceroses ,', '. What', 'too ridiculous', 'too overplayed', ', so', 'them acquaintances', \"was n't\", 'King 1', 'A clan', 'Whoopi Goldberg', '. Pretty', 'Dwarfs ,', '. Way', 'to hide', 'the place', 'than friends.<br', 'took a', 'leave .', 'This one', 'being funny', 'as Ed', 'won ,', 'kids will', 'heading down', 'joins his', 'SPOILERS!!!]<br /><br', 'from Disney', 'Ed arrived', 'warthog named', 'they gaze', 'So they', 'centered meerkat', 'to give', 'better place', 'to trap', 'Timon deathly', '. This', 'Rock lifting', 'Peter Pan', 'with Timon', \"it 's\", 'gas and', 'finish the', 'of wildebeests', 'that The', 'point of', 'out bowling', 'frivilous .', 'Robert Guillame', '> Timon', 'to honor', 'Genie ,', \"'s 1\", ', leaving', 'flatulence problem', ', rather', 'Rafiki \"', 'after Part', 'Cheech Marin', 'place right', 'I do', 'see .', 'father figures', 'Then one', 'a II', 'decides to', 'reach land', 'of meerkats', 'and Part', 'away from', 'Rafiki want', ', and', 'chose to', 'his friends', 'old home', 'Goldberg as', 'II .', 'due to', 'up .', 'newfound paradise', 'suddenly Mother', 'took it', 'stampede of', ', until', 'Simba won', 'silly .', 'That did', 'stream .', 'all is', ', Matthew', 'come to', ', Dumbo', 'to stay', ', it', \"n't care\", 'He chose', '1/2 was', 'Bonzai ,', 'Was this', 'BEFORE the', 'New to', \"she 'd\", 'duty ;', 'they suddenly', 'Max and', 'the new', 'going on', 'Kavner of', 'movie .', 'soon flung', 'Jerry Stiller', 'again .', 'a serious', 'then they', 'getting homesick', 'on him', 'the clan', 'paradise .', 'deathly ill.<br', ', Peter', 'then .', 'No ,', 'kissed his', '. Back', 'Shenzi ,', 'very far', 'Simba and', \"'s mother\", 'worked .', 'Lost Boys', 'serious story', 'Julie Kavner', 'noses ;', 'sometimes Timon', 'because it', 'made a', 'then all', 'Mickey Mouse', 'So he', 'when suddenly', 'King .', 'hole where', '. Suddenly', 'Rafiki ,', \"could n't\", 'Max ,', 'Adult Simba', '. However', 'ridiculous and', 'Sabella as', 'could see', ', Ernie', 'to look', 'the hyenas', '. So', 'home which', 'Uncle Max', 'flung down', 'CONTAINS SPOILERS!!!]<br', 'up the', 'miles away', 'mom and', ', Cheech', 'Pan ,', 'not Timon', 'anyway ,', 'upon himself', 'that meant', 'under their', 'Broddrick as', 'a pompous', 'of bug', 'become father', 'go away', 'running series', 'the animals', '\" some', 'what that', '\" by', 'so he', ', Robert', '\" Hakuna', ', Simba', '> Well', 'arrive at', 'on ?', 'Scar is', 'to a', 'Then all', 'problem .', 'meerkats go', ', Mother', 'trees and', 'the present', 'somewhere .', 'longer being', 'friends.<br /><br', 'sentry duty', 'rest of', 'But then', 'and Rafiki', 'Boys ,', 'Cummings as', 'just way', 'contests .', 'as I', 'into the', '[ CONTAINS', 'Pumbaa just', 'cast is', 'III ,', '. Hence', 'say that', 'Pumbaa try', 'the kids', 'story really', 'Max .', 'he kissed', 'arts of', 'knew she', 'clue what', ', I', 'mother took', 'and Timon', 'will love', 'in the', ', hyenas', 'movie when', 'Pumbaa go', 'While Simba', '\" .', 'elephants and', 'antelopes ,', 'plain animals', 'really began', 'expect from', 'Goofy ,', 'nearly killed', 'watch it', 'Pumbaa ,', 'was all', 'overplayed .', 'and started', 'eye could', 'on sentry', 'or hyenas', 'It worked', 'worst digger', 'after him', 'Well ,', '/ >', 'was going', 'Jasmine.<br /><br', 'mom goodbye', 'Dumbo ,', 'falls as', '. It', ', Timon', 'other meerkats', ', rhinoceroses', 'I recommend', 'a warthog', \"TV 's\", 'cover their', 'try an', 'as Shenzi', \"'s old\", 'but each', 'original voice', 'Mother and', 'ill.<br /><br', 'of no', 'him to', 'Two is', 'runs away', 'dig a', 'Pumbaa join', 'the Lost', 'in .', 'themselves heading', ', this', 'the rest', 'mother and', 'alone due', 'had dreams', 'soon ,', 'for buzzards', 'and belching', 'sequel makers.<br', 'zebras ,', 'die hard', 'It takes', 'Simba was', 'from hyenas', 'a large', 'Simba runs', 'large stampede', 'because there', 'good as', 'Pumbaa in', 'Hence ,', 'Timon to', 'They pressed', 'and Ed', 'suddenly find', 'is back', 'him ,', 'and made', \"'s ,\", 'that is', 'new king', 'taught him', 'is Hakuna', 'the beginning', 'wanted Timon', ', wildebeests', ': Hakuna', 'contest .', 'series The', 'which was', 'just shook', 'until Rafiki', 'digger in', 'conclusion ,', 'land ,', 'it to', '. Just', 'just wanted', 'II is', 'his mom', 'to them', 'Timon in', 'watching The', 'Lion King', 'just silly', 'Pumbaa finish', 'a snail', 'before Simba', 'were bowing', 'soon arrive', 'acquaintances ,', 'Timon suddenly', 'meerkats burrowed', 'born .', 'incessant singing', 'their noses', '/><br />-', 'beautiful :', 'recommend it', 'Ernie Sabella', 'at Pride', 'Max dig', '/><br /', '. In', 'The worst', 'as good', 'sees .', 'Seven Dwarfs', 'Rafiki atop', 'The original', 'Mother ,', \"They 're\", '> They', \"'s mom\", 'some sense', 'overplayed to', 'himself ,', 'as Pumbaa', 'Part II', '. Two', 'battles Scar', 'it again', 'a strange', 'out there', ', antelopes', 'join up', 'then saw', 'love it', 'so I', 'takes on', 'but Timon', \"Timon 's\", 'self -', 'look beyond', 'who was', 'Timon declared', 'he started', 'they go', 'Timon did', '. When', 'the eye', 'place after', 'The Lion', 'are just', 'wing and', 'at \"', 'was a', 'and silly', ', the', 'Guillame as', '. New', 'or a', 'water falls', 'to die', 'just annoying', ', Whoopi', 'arrived and', '\" talked', 'dreams of', '. But', 'Nathan Lane', 'bowing to', 'They teach', \"'s (\", '. Then', 'find themselves', 'Too )', 'suggested putting', 'and Pumbaa', 'began .', 'where all', 'had no', 'this movie', 'Max just', 'go out', 'was beautiful', 'atop Pride', 'of the', 'Hakuna Matata', 'gaze around', ', Goofy', '. Timon', 'of TV', 'pressed on', 'the birth', 'shook his', 'Duck ,', 'Simba .', 'Marin as', 'I swear', 'Snow White', \"'s childhood\", 'clan was', \"'re overplayed\", 'other plain', 'him the', '. His', 'Timon .', 'as Uncle', 'Matata .', 'and Max', 'had hoped', 'eating and', ', March', 'it .', 'he continued', 'saw Rafiki', 'Max arrive', 'buzzards one', 'right after', 'did it', 'strange phrase', 'masterpiece .', 'a ways', 'when they', 'him but', 'all the', 'this to', 'Pride Rock', 'same hole', 'I must', 'story with', 'Rafiki :', 'Rafiki .', 'assortment of', '. I', 'thrown in', 'get very', 'LAST Lion', 'teenage Simba', 'paradise that', '1/2 \"', 'on and', 'far too', 'King movie', 'and many', 'it upon', 'is soon', 'break up', 'Simpsons as', '. Was', 'Ed in', '. \"', 'mockery of', 'recommend this', 'hard Lion', 'go back', 'light comedy', 'him under', 'was miles', 'makers.<br /><br', 'around at', '. They', 'Simba ,', 'something he', 'a flatulence', 'to go', 'live with', 'hope this', 'I had', ', sometimes', 'the zebras', 'bow .', 'also be', 'King was', 'the friendship', 'to leave', 'was just', 'named the', 'was too', ') long', ', Donald', \"we 've\", \"n't as\", 'the air', 'race after', 'enough .', 'do Mickey', 'devoured by', 'Jim Cummings', ', elephants', 'after a', 'a bigger', 'passed gas', 'to watch', 'Simba battles', 'the Seven', 'of a', 'down stream', '; Timon', 'had passed', 'movie is', 'he learned', 'Matata \"', 'comedy thrown', ', Snow', 'arrive then', 'childhood friend', 'They soon', 'new homes', 'want to', 'air something', 'their newfound', 'Pumbaa are', 'gathered .', 'King III', '. A', ': Nathan', '1 1/2', 'afternoon when', 'must say', 'away .', 'go live', 'down the', 'as Timon', 'Timon ,', 'learned from', 'I hope', 'funny ,', 'annoying .', 'voice cast', 'It had', 'Matthew Broddrick', 'They take', 'as Rafiki', 'hyenas or', 'figures .', \"Simba 's\", 'by the', 'to incessant', 'of new', 'beyond what', 'killed poor', 'to when', 'What was', 'and it', 'Rock .', 'the cast', 'So anyway', 'Part I', 'What we', 'the paradise', 'However the', 'swear ,', 'Timon took', 'Mouse ,', ', but', 'already is', 'hyenas .', 'animals were', 'Suddenly ,', '. Nala', ', Jim', ', Bonzai', 'homes ,', 'fans because', 'named Pumbaa', 'as Adult', ', just', 'Timon decides', 'friendship .', 'hyenas Shenzi', 'in a', ', Aladdin', 'back .', 'Timon named', 'far as', 'Timon then', 'The Simpsons', 'there somewhere', 'who taught', 'cast are', 'King fans', 'slurping contest', 'to cover', 'Stiller as', 'to expect', 'teach him', 'arrived .', 'silly and', 'Lane as', '. While', 'present ,', 'sense into', 'Hare ,', 'stay at', 'animals had', 'Scar ,', 'and nearly', 'then ,', 'the point', 'Pumbaa and', 'the arts', \"n't II\", 'trap hyenas', 'hole to', 'homesick .', 'his head', '. He', 'March Hare', 'is well', 'from Rafiki', 'original Lion', 'one was', 'meerkat named', 'He did', 'suddenly run', 'Mad Hatter', 'long running', 'phrase he', '. Scar', 'as the', 'well .', 'friend Nala', 'Timon on', 'what he', 'give up', 'at their', 'a bow', '( Too', 'His mother', 'Hatter ,', 'II because', 'Timon and', 'met Rafiki', 'into him', 'Disney sequel', 'no clue', 'care .', 'as Bonzai', 'the story', 'day ,', 'and Jerry', 'from Pride', ', Pumbaa', 'head .', 'be the', 'of it', 'pity on', 'way too', 'and better', 'ways away', 'started getting', 'one afternoon', 'goodbye and', 'Way back', 'wildebeests ,', 'bug eating', 'their wing', 'animals took', 'and water', '? Timon', 'and become', ': trees', 'a mockery', 'Nala and', 'lifting into', '1/2 .', 'no longer', 'meant so', 'but not', 'the same', \"do n't\", 'I and', '. The', 'belching contests', 'started off', 'king ?', 'was born', \"is n't\", 'Donald Duck', 'the movie', 'are watching', 'and met', 'to the', 'as far', 'before he', 'up then', 'Mother suggested', 'Pumbaa had', 'Aladdin ,', 'hoped .', 'Bonzai and', 'So do', 'meerkats just', 'putting Timon', 'bowling for', '? No', 'place out', 'Pretty soon', 'burrowed underground', 'and Uncle', 'where he', 'is devoured', 'are Julie', \"n't see\"]\n",
            ">>> label : neg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slloOJ9WO8LI",
        "outputId": "879b15c9-b06f-4b1d-edfe-b914f5da9832"
      },
      "source": [
        "%%time\r\n",
        "\r\n",
        "# build vocabulary\r\n",
        "MAX_VOCAB_SIZE = 25_000\r\n",
        "TEXT.build_vocab(train_data,\r\n",
        "                 max_size = MAX_VOCAB_SIZE,\r\n",
        "                 vectors='glove.6B.100d',\r\n",
        "                 unk_init=torch.Tensor.normal_)\r\n",
        "LABEL.build_vocab(train_data)\r\n",
        "\r\n",
        "print('\\n')\r\n",
        "print(f\">>> Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\r\n",
        "print(f\">>> Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\r\n",
        "print(f\">>> Top 20 common tokens :{TEXT.vocab.freqs.most_common(20)}\")\r\n",
        "print()\r\n",
        "print('<itos and stoi>')\r\n",
        "print('>>> itos :', TEXT.vocab.itos[:10])\r\n",
        "print('>>> stoi :', LABEL.vocab.stoi)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            ">>> Unique tokens in TEXT vocabulary: 25002\n",
            ">>> Unique tokens in LABEL vocabulary: 2\n",
            ">>> Top 20 common tokens :[('the', 201483), (',', 191071), ('.', 165466), ('and', 108901), ('a', 108890), ('of', 99815), ('to', 92913), ('is', 76451), ('in', 61037), ('I', 54352), ('it', 53477), ('that', 49157), ('\"', 43935), (\"'s\", 43037), ('this', 42289), ('-', 36343), ('/><br', 35252), ('was', 34941), ('as', 30317), ('movie', 29866)]\n",
            "\n",
            "<itos and stoi>\n",
            ">>> itos : ['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n",
            ">>> stoi : defaultdict(None, {'neg': 0, 'pos': 1})\n",
            "CPU times: user 9.25 s, sys: 792 ms, total: 10 s\n",
            "Wall time: 10.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcqkAT0d41Op",
        "outputId": "1db57ecf-de26-42c6-d636-6514cc9a1806"
      },
      "source": [
        "# create the iterators\r\n",
        "BATCH_SIZE = 64\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\r\n",
        "    datasets=(train_data, valid_data, test_data),\r\n",
        "    batch_size=BATCH_SIZE,\r\n",
        "    device=device,\r\n",
        "    sort_within_batch=True\r\n",
        ")\r\n",
        "\r\n",
        "device, train_iterator, len(train_iterator), len(train_iterator)/BATCH_SIZE"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(device(type='cpu'),\n",
              " <torchtext.legacy.data.iterator.BucketIterator at 0x7fef77a6b2d0>,\n",
              " 274,\n",
              " 4.28125)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U0Ulm3zW3hX"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA-hnGUOW4C8",
        "outputId": "f6508d97-f179-4077-a6b6-b5dd75908d1e"
      },
      "source": [
        "class FastText(nn.Module):\r\n",
        "    def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\r\n",
        "        super().__init__()\r\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=pad_idx)\r\n",
        "        self.fc = nn.Linear(in_features=embedding_dim, out_features=output_dim)\r\n",
        "        \r\n",
        "        \r\n",
        "    def forward(self, text):\r\n",
        "        # text = [sent len, batch size]\r\n",
        "        # print(text.shape)  ########################## DEBUG #########################\r\n",
        "\r\n",
        "        embedded = self.embedding(text)\r\n",
        "        # embedded = [sent len, batch size, emb dim]\r\n",
        "        # print(embedded.shape)  ########################## DEBUG #########################\r\n",
        "\r\n",
        "        embedded = embedded.permute(1, 0, 2)\r\n",
        "        # embedded = [batch size, sent len, emb dim]\r\n",
        "        # print(embedded.shape)  ########################## DEBUG #########################\r\n",
        "\r\n",
        "        pooled = F.avg_pool2d(input=embedded, kernel_size=(embedded.shape[1], 1)).squeeze(1)\r\n",
        "        # pooled = [batch size, emb dim]\r\n",
        "        # print(pooled.shape)  ########################## DEBUG #########################\r\n",
        "        \r\n",
        "        output = self.fc(pooled)\r\n",
        "        # output = [batch size, output dim]\r\n",
        "        # print(output.shape)  ########################## DEBUG #########################\r\n",
        "        \r\n",
        "        return output\r\n",
        "\r\n",
        "\r\n",
        "INPUT_DIM = len(TEXT.vocab)\r\n",
        "EMBEDDING_DIM = 100  # 50-250\r\n",
        "OUTPUT_DIM = 1       # No of labels\r\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]  # '<pad>' -> 1\r\n",
        "\r\n",
        "model = FastText(vocab_size=INPUT_DIM, embedding_dim=EMBEDDING_DIM, output_dim=OUTPUT_DIM, pad_idx=PAD_IDX).to(device)\r\n",
        "num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "print(f'>>> The model has {num_parameters:,} trainable parameters')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> The model has 2,500,301 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0ONn_QAfedx"
      },
      "source": [
        "# # Check output tensor shape through whole model (with sample input text)\r\n",
        "# ## 1. generate sample text\r\n",
        "# text = torch.randint(size=(3, 9), \r\n",
        "#                           low=0, high=len(TEXT.vocab), dtype=torch.long, device=device)\r\n",
        "# text_lengths = torch.LongTensor([text.shape[0]] * text.shape[1])\r\n",
        "# print(f'>>> text : {text.shape}, text_lengths : {text_lengths.shape} (value : {text_lengths})')\r\n",
        "\r\n",
        "# ## 2. Generate test model\r\n",
        "# test_model = FastText(vocab_size=INPUT_DIM, embedding_dim=EMBEDDING_DIM, output_dim=OUTPUT_DIM, pad_idx=PAD_IDX).to(device)\r\n",
        "\r\n",
        "# ## 3. Check shapes through each layer\r\n",
        "# torch.sigmoid(test_model(text))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O77VApqJCin",
        "outputId": "40afe51d-0114-4d69-f3dd-d73825742403"
      },
      "source": [
        "# check original weights\r\n",
        "original_weights = model.embedding.weight.data.clone().detach()\r\n",
        "print('>>> original initial weights :\\n', original_weights.shape)\r\n",
        "print(original_weights)\r\n",
        "\r\n",
        "# replace initial weights of embedding layer with pre-trained embeddings\r\n",
        "model.embedding.weight.data.copy_(TEXT.vocab.vectors)\r\n",
        "\r\n",
        "# replace initial weights o unk & pad tokens with zeros\r\n",
        "UNK_IDX = TEXT.vocab.unk_index  # '<unk>' -> 0\r\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\r\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\r\n",
        "\r\n",
        "# check replaced weights\r\n",
        "print('\\n>>> replaced initial weights :\\n', model.embedding.weight.data.shape)\r\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> original initial weights :\n",
            " torch.Size([25002, 100])\n",
            "tensor([[ 0.0602, -1.4128,  0.5588,  ..., -0.6106, -1.8935, -0.5687],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0518,  1.7830,  1.1010,  ...,  0.0764, -0.5569,  1.1415],\n",
            "        ...,\n",
            "        [-0.4844, -1.5636,  0.3106,  ..., -0.2987, -0.9592,  0.3852],\n",
            "        [ 0.5401, -2.3420,  1.5356,  ..., -1.4318, -0.3638, -0.4516],\n",
            "        [-0.4139,  1.0186, -0.0582,  ..., -0.9986,  0.0448,  0.8660]])\n",
            "\n",
            ">>> replaced initial weights :\n",
            " torch.Size([25002, 100])\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "        ...,\n",
            "        [ 0.5043, -0.4016,  0.3739,  ..., -0.6785, -0.6437,  0.5768],\n",
            "        [ 0.2256, -1.5004, -0.4065,  ..., -0.4732,  1.0807,  1.9720],\n",
            "        [-1.0441, -0.7261, -0.2653,  ...,  1.4327,  0.9158,  0.1065]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xpKz7ARNYWZ"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phyhzJXjNinL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k8qVtT0Nab9"
      },
      "source": [
        "## Load and Test saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnPiB77zNi8V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmVK5HqoNeHX"
      },
      "source": [
        "## Predict New Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI-RYO4pNjXQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}