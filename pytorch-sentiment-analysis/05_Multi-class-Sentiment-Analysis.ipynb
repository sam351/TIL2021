{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_Multi-class-Sentiment-Analysis",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClGpy3rvL1_5",
        "outputId": "cb92e16b-18db-4e1d-e573-c1778531b1bf"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n",
        "from torchtext.legacy import data, datasets\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import spacy\n",
        "\n",
        "\n",
        "torch.__version__, torchtext.__version__, spacy.__version__, np.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('1.8.1+cu101', '0.9.1', '2.2.4', '1.19.5')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I39JKY87WzB9"
      },
      "source": [
        "## Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xmBt5IAbCKq",
        "outputId": "21195f14-1fc3-4ca4-cf2b-6482ac4099d6"
      },
      "source": [
        "%%time\n",
        "\n",
        "# set random seed for reproducibility\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# download and split dataset (train, valid, test)\n",
        "TEXT = data.Field(\n",
        "    tokenize='spacy', tokenizer_language='en_core_web_sm', batch_first=True\n",
        ")\n",
        "LABEL = data.LabelField()\n",
        "\n",
        "train_data, test_data = datasets.TREC.splits(TEXT, LABEL, fine_grained=False)\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(SEED))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading train_5500.label\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "train_5500.label: 100%|██████████| 336k/336k [00:00<00:00, 1.34MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading TREC_10.label\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TREC_10.label: 100%|██████████| 23.4k/23.4k [00:00<00:00, 378kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1.15 s, sys: 92.2 ms, total: 1.24 s\n",
            "Wall time: 3.83 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es19mu23RL4j",
        "outputId": "5b011756-d4cd-4d72-ecd4-4997d92f0298"
      },
      "source": [
        "# check the type and size of dataset\n",
        "print(f'>>> type : {type(train_data)}')\n",
        "print(f'>>> Number of training examples: {len(train_data)}')    # 3816 (64.1%)\n",
        "print(f'>>> Number of validation examples: {len(valid_data)}')  # 1636 (27.5%)\n",
        "print(f'>>> Number of testing examples: {len(test_data)}')      # 500  (8.4%)\n",
        "print()\n",
        "\n",
        "# check sample data\n",
        "tmp_data = train_data[-1]\n",
        "tmp_dict = vars(tmp_data)\n",
        "print('< sample data >')\n",
        "print('>>> type :', type(tmp_data))\n",
        "for key in tmp_dict:\n",
        "  print(f'>>> {key} : {tmp_dict[key]}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> type : <class 'torchtext.legacy.data.dataset.Dataset'>\n",
            ">>> Number of training examples: 3816\n",
            ">>> Number of validation examples: 1636\n",
            ">>> Number of testing examples: 500\n",
            "\n",
            "< sample data >\n",
            ">>> type : <class 'torchtext.legacy.data.example.Example'>\n",
            ">>> text : ['What', 'is', 'a', 'Cartesian', 'Diver', '?']\n",
            ">>> label : DESC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62Xn5X42hyTT",
        "outputId": "3098db2d-3ed8-4bdc-f2f7-ffa624cd894c"
      },
      "source": [
        "%%time\n",
        "\n",
        "# build vocabulary\n",
        "MAX_VOCAB_SIZE = 25_000\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print('\\n')\n",
        "print(f\">>> Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\">>> Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
        "print(f\">>> Top 20 common tokens :{TEXT.vocab.freqs.most_common(20)}\")\n",
        "print()\n",
        "print('<itos and stoi>')\n",
        "print('>>> itos :', TEXT.vocab.itos[:10])\n",
        "print('>>> stoi :', LABEL.vocab.stoi)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:58, 4.82MB/s]                           \n",
            "100%|█████████▉| 398880/400000 [00:14<00:00, 26821.33it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            ">>> Unique tokens in TEXT vocabulary: 7503\n",
            ">>> Unique tokens in LABEL vocabulary: 6\n",
            ">>> Top 20 common tokens :[('?', 3743), ('the', 2502), ('What', 2265), ('is', 1165), ('of', 1069), ('in', 791), ('a', 691), ('`', 589), ('How', 512), (\"'s\", 494), ('was', 449), ('to', 431), (',', 400), ('Who', 398), ('for', 332), ('and', 303), ('are', 294), ('does', 284), (\"''\", 283), ('did', 265)]\n",
            "\n",
            "<itos and stoi>\n",
            ">>> itos : ['<unk>', '<pad>', '?', 'the', 'What', 'is', 'of', 'in', 'a', '`']\n",
            ">>> stoi : defaultdict(None, {'HUM': 0, 'ENTY': 1, 'DESC': 2, 'NUM': 3, 'LOC': 4, 'ABBR': 5})\n",
            "CPU times: user 35.2 s, sys: 5.35 s, total: 40.5 s\n",
            "Wall time: 3min 41s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "HcaDTK5shyWB",
        "outputId": "5f5a1fe0-a1dd-460b-d5f1-9e3564b6d1bc"
      },
      "source": [
        "# create the iterators\n",
        "BATCH_SIZE = 64\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device\n",
        ")\n",
        "\n",
        "display(device, type(train_iterator), len(train_iterator), len(train_data)/BATCH_SIZE)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "torchtext.legacy.data.iterator.BucketIterator"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "59.625"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U0Ulm3zW3hX"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aql7UrkMpaHO"
      },
      "source": [
        "# function for model summary\n",
        "def model_summary(model):\n",
        "  num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f'>>> The model has {num_parameters:,} trainable parameters')\n",
        "  print(model)\n",
        "\n",
        "\n",
        "# hyper-parameters for CNN model\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100  # 50-250\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]  # '<pad>' -> 1\n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [2, 3, 4]\n",
        "OUTPUT_DIM = len(LABEL.vocab)  # No of labels\n",
        "DROPOUT = 0.5"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA-hnGUOW4C8"
      },
      "source": [
        "# # 1-Dimensional CNN model for flexible convolutional layers\n",
        "# class CNN1d(nn.Module):\n",
        "#   def __init__(self, vocab_size, embedding_dim, pad_idx, output_dim, n_filters, filter_sizes, dropout):\n",
        "#     super().__init__()\n",
        "#     self.embedding = nn.Embedding(vocab_size, embedding_dim, pad_idx)\n",
        "#     self.convs = nn.ModuleList([nn.Conv1d(in_channels=embedding_dim, out_channels=n_filters, kernel_size=fs) for fs in filter_sizes])\n",
        "#     self.droptout = nn.Dropout(dropout)\n",
        "#     self.fc = nn.Linear(n_filters * len(filter_sizes), output_dim)\n",
        "\n",
        "#   def forward(self, text_ts):\n",
        "#     # text_ts = [batch size, sent len]\n",
        "\n",
        "#     embedded = self.embedding(text_ts).permute(0, 2, 1)\n",
        "#     # embedded = [batch size, emb dim, sent len]\n",
        "\n",
        "#     conved = [conv(embedded) for conv in self.convs]\n",
        "#     # conved = [batch size, n_filters, sent len - filter_sizes[0] + 1]\n",
        "\n",
        "#     pooled = [F.max_pool1d(c, c.shape[-1]).squeeze(-1) for c in conved]\n",
        "#     # pooled = [batch size, n_filters]\n",
        "\n",
        "#     cat = self.droptout(torch.cat(pooled, dim=1))\n",
        "#     # cat = [batch size, n_filters * len(filter_sizes)]\n",
        "\n",
        "#     out = self.fc(cat)\n",
        "#     # out = [batch size, 1]\n",
        "\n",
        "#     return out"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47gb-Q3zFKgt"
      },
      "source": [
        "# # generate model & summary\n",
        "# model = CNN1d(vocab_size=INPUT_DIM, embedding_dim=EMBEDDING_DIM, pad_idx=PAD_IDX, n_filters=N_FILTERS,\n",
        "#             filter_sizes=FILTER_SIZES, output_dim=OUTPUT_DIM, dropout=DROPOUT).to(device)\n",
        "# model_summary(model)\n",
        "\n",
        "\n",
        "# # test tensor shape in each step\n",
        "# print('\\n< shape after each layer >')\n",
        "# ts = torch.randint(low=1, high=250, size=(2, 12)).to(device)\n",
        "# print('>>> text_ts =', ts.shape)\n",
        "# ts = model.embedding(ts).permute(0, 2, 1)\n",
        "# print('>>> embedded =', ts.shape)\n",
        "# ts = model.convs[0](ts)\n",
        "# print('>>> conved[0] =', ts.shape)\n",
        "# ts = F.max_pool1d(ts, ts.shape[-1]).squeeze(-1)\n",
        "# print('>>> pooled[0] =', ts.shape)\n",
        "# ts = torch.cat([ts]*3, dim=1)\n",
        "# print('>>> cat =', ts.shape)\n",
        "# ts = model.fc(ts)\n",
        "# print('>>> out =', ts.shape)\n",
        "# ts"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kct2L5Eljq3-"
      },
      "source": [
        "# CNN model for flexible convolutional layers\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, pad_idx, n_filters, filter_sizes, output_dim, dropout):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, pad_idx)\n",
        "    # self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.convs = nn.ModuleList([nn.Conv2d(1, n_filters, (fsize, embedding_dim)) for fsize in filter_sizes])\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.fc = nn.Linear(n_filters*len(filter_sizes), output_dim)\n",
        "  \n",
        "  def forward(self, text_ts):\n",
        "    # text_ts = [batch size, sent len]\n",
        "    \n",
        "    embedded = self.embedding(text_ts).unsqueeze(1)\n",
        "    # [batch size, 1, sent len, emb dim]\n",
        "\n",
        "    conved = [F.relu(conv(embedded)).squeeze(-1) for conv in self.convs]\n",
        "    # conved = [batch size, n_filters, sent len - filter size + 1]\n",
        "\n",
        "    pooled = [F.max_pool1d(conv, conv.shape[-1]).squeeze(-1) for conv in conved]\n",
        "    # pooled = [batch size, n_filters]\n",
        "\n",
        "    cat = self.dropout(torch.cat(pooled, dim=1))\n",
        "    # cat = [batch size, n_filters * len(filter_sizes)]\n",
        "\n",
        "    out = self.fc(cat)\n",
        "    # out = [batch size, output dim]\n",
        "\n",
        "    return out"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOH7cgxzqDDR",
        "outputId": "d609ee6f-5e0b-482d-d99e-40c251424fd9"
      },
      "source": [
        "# generate model & summary\n",
        "model = CNN(vocab_size=INPUT_DIM, embedding_dim=EMBEDDING_DIM, pad_idx=PAD_IDX, n_filters=N_FILTERS,\n",
        "            filter_sizes=FILTER_SIZES, output_dim=OUTPUT_DIM, dropout=DROPOUT).to(device)\n",
        "model_summary(model)\n",
        "\n",
        "# test tensor shape in each step\n",
        "print('\\n< shape after each layer >')\n",
        "ts = torch.randint(low=1, high=250, size=(2, 12)).to(device)\n",
        "print('>>> text_ts =', ts.shape)\n",
        "ts = model.embedding(ts).unsqueeze(1)\n",
        "print('>>> embedded =', ts.shape)\n",
        "ts = F.relu(model.convs[0](ts)).squeeze(-1)\n",
        "print('>>> conved[0] =', ts.shape)\n",
        "ts = F.max_pool1d(ts, ts.shape[-1]).squeeze(-1)\n",
        "print('>>> pooled[0] =', ts.shape)\n",
        "ts = torch.cat([ts]*3, dim=1)\n",
        "print('>>> cat =', ts.shape)\n",
        "ts = model.fc(ts)\n",
        "print('>>> out =', ts.shape)\n",
        "ts"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> The model has 842,406 trainable parameters\n",
            "CNN(\n",
            "  (embedding): Embedding(7503, 100, padding_idx=1)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv2d(1, 100, kernel_size=(2, 100), stride=(1, 1))\n",
            "    (1): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
            "    (2): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
            "  )\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=300, out_features=6, bias=True)\n",
            ")\n",
            "\n",
            "< shape after each layer >\n",
            ">>> text_ts = torch.Size([2, 12])\n",
            ">>> embedded = torch.Size([2, 1, 12, 100])\n",
            ">>> conved[0] = torch.Size([2, 100, 11])\n",
            ">>> pooled[0] = torch.Size([2, 100])\n",
            ">>> cat = torch.Size([2, 300])\n",
            ">>> out = torch.Size([2, 6])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.5813, -0.6865, -0.0946, -0.2749, -0.3098, -0.5300],\n",
              "        [ 0.9474, -0.3289,  0.1229, -0.6886, -0.6367, -0.3558]],\n",
              "       device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "docsyRD_FKmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "525ae489-f646-4ca9-b2ad-0a9648bb4d52"
      },
      "source": [
        "# check original weights\n",
        "original_weights = model.embedding.weight.data\n",
        "print('>>> original initial weights :\\n', original_weights.shape)\n",
        "print(original_weights)\n",
        "\n",
        "# replace initial weights of embedding layer with pre-trained embeddings\n",
        "model.embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
        "\n",
        "# replace initial weights of unk & pad tokens with zeros\n",
        "UNK_IDX = TEXT.vocab.unk_index  # '<unk>' -> 0\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "# check replaced weights\n",
        "print('\\n>>> replaced initial weights :\\n', model.embedding.weight.data.shape)\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> original initial weights :\n",
            " torch.Size([7503, 100])\n",
            "tensor([[-0.2648, -0.3553, -1.6073,  ...,  0.4329, -1.8583,  1.8540],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.3440, -0.2418, -0.9076,  ..., -0.1531,  2.3643, -0.9868],\n",
            "        ...,\n",
            "        [ 1.3123, -0.3104, -0.4390,  ..., -1.7139,  1.4558,  0.2811],\n",
            "        [-1.0843,  0.6616,  0.7179,  ...,  1.6740,  0.2126,  0.3817],\n",
            "        [ 0.8596,  0.4862, -0.4411,  ...,  1.3099,  1.3882, -1.2138]],\n",
            "       device='cuda:0')\n",
            "\n",
            ">>> replaced initial weights :\n",
            " torch.Size([7503, 100])\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.1638,  0.6046,  1.0789,  ..., -0.3140,  0.1844,  0.3624],\n",
            "        ...,\n",
            "        [-0.3110, -0.3398,  1.0308,  ...,  0.5317,  0.2836, -0.0640],\n",
            "        [ 0.0091,  0.2810,  0.7356,  ..., -0.7508,  0.8967, -0.7631],\n",
            "        [ 0.4306,  1.2011,  0.0873,  ...,  0.8817,  0.3722,  0.3458]],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgkDwlvvZAC1"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ5N8590p9iU"
      },
      "source": [
        "# function for calculating categorical accuracy\n",
        "def categorical_accuracy(preds, y):\n",
        "  pred_class = preds.argmax(dim=1, keepdim=True)\n",
        "  acc = (pred_class == y.view_as(pred_class)).float().mean()\n",
        "  return acc\n",
        "\n",
        "\n",
        "# function for training in each epoch\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  model.train()\n",
        "\n",
        "  for batch in iterator:\n",
        "    pred = model(batch.text)\n",
        "    loss = criterion(pred, batch.label)\n",
        "    acc = categorical_accuracy(pred, batch.label)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss\n",
        "    epoch_acc  += acc\n",
        "\n",
        "  return epoch_loss/len(iterator), epoch_acc/len(iterator)\n",
        "\n",
        "\n",
        "# function for evaluation (vaildation or test)\n",
        "def evaluate(model, iterator, criterion):\n",
        "  loss = 0\n",
        "  acc = 0\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in iterator:\n",
        "      pred = model(batch.text)\n",
        "      loss += criterion(pred, batch.label)\n",
        "      acc += categorical_accuracy(pred, batch.label)\n",
        "  return loss/len(iterator), acc/len(iterator)\n",
        "\n",
        "\n",
        "# function for calculating min, sec from start & end time\n",
        "def epoch_time(start_time, end_time):\n",
        "  elapsed_time = end_time - start_time\n",
        "  elapsed_mins = int(elapsed_time/60)\n",
        "  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "  return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CN7kGTj7ZNhW",
        "outputId": "a9f4e3ac-eace-403c-b395-789c9f6dfb76"
      },
      "source": [
        "%%time\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "outfile_dir = 'tut5-model.pt'\n",
        "\n",
        "n_epoch = 5\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(n_epoch):\n",
        "  start_time = time.time()\n",
        "  train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "  valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "  end_time = time.time()\n",
        "\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "  print(f'Epoch : {epoch+1:02}  |  Epoch time : {epoch_mins}m {epoch_secs}s')\n",
        "  print(f'\\tTrain Loss : {train_loss:.3f}  |  Train Acc : {train_acc*100:.2f}%')\n",
        "  print(f'\\tValid Loss : {valid_loss:.3f}  |  Valid Acc : {valid_acc*100:.2f}%')\n",
        "  \n",
        "  if valid_loss < best_val_loss:\n",
        "    best_val_loss = valid_loss\n",
        "    torch.save(model.state_dict(), outfile_dir)\n",
        "    print('>>> Saved best model in epoch', epoch+1)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 01  |  Epoch time : 0m 0s\n",
            "\tTrain Loss : 1.305  |  Train Acc : 48.63%\n",
            "\tValid Loss : 0.943  |  Valid Acc : 66.87%\n",
            ">>> Saved best model in epoch 1\n",
            "Epoch : 02  |  Epoch time : 0m 0s\n",
            "\tTrain Loss : 0.869  |  Train Acc : 67.83%\n",
            "\tValid Loss : 0.748  |  Valid Acc : 74.75%\n",
            ">>> Saved best model in epoch 2\n",
            "Epoch : 03  |  Epoch time : 0m 0s\n",
            "\tTrain Loss : 0.658  |  Train Acc : 77.44%\n",
            "\tValid Loss : 0.631  |  Valid Acc : 78.27%\n",
            ">>> Saved best model in epoch 3\n",
            "Epoch : 04  |  Epoch time : 0m 0s\n",
            "\tTrain Loss : 0.499  |  Train Acc : 83.22%\n",
            "\tValid Loss : 0.554  |  Valid Acc : 80.70%\n",
            ">>> Saved best model in epoch 4\n",
            "Epoch : 05  |  Epoch time : 0m 0s\n",
            "\tTrain Loss : 0.387  |  Train Acc : 87.96%\n",
            "\tValid Loss : 0.505  |  Valid Acc : 81.95%\n",
            ">>> Saved best model in epoch 5\n",
            "CPU times: user 1.49 s, sys: 332 ms, total: 1.82 s\n",
            "Wall time: 1.94 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HV7KVv5a4zp"
      },
      "source": [
        "## Load and Test saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-npGd975ZNm9",
        "outputId": "dac1f047-4e8f-45b5-d231-7999a69229d8"
      },
      "source": [
        "%%time\n",
        "\n",
        "# function for checking weight equality\n",
        "def check_weights(model1, model2):\n",
        "  flag = True\n",
        "  for p1, p2 in zip(model1.parameters(), model2.parameters()):\n",
        "    if not p1.data.equal(p2.data):\n",
        "      flag = False\n",
        "  return flag\n",
        "\n",
        "# load model\n",
        "loaded_model = CNN(vocab_size=INPUT_DIM, embedding_dim=EMBEDDING_DIM, pad_idx=PAD_IDX, n_filters=N_FILTERS,\n",
        "                   filter_sizes=FILTER_SIZES, output_dim=OUTPUT_DIM, dropout=DROPOUT).to(device)\n",
        "loaded_model.load_state_dict(torch.load(outfile_dir))\n",
        "\n",
        "# check weight equality\n",
        "print('>>> All weights are equal') if check_weights(model, loaded_model) else print('>>> WARNING!!! Not equal wieghts!')\n",
        "\n",
        "# test model performance\n",
        "test_loss, test_acc = evaluate(loaded_model, test_iterator, criterion)\n",
        "print(f'>>> Test Loss : {test_loss:.3f}  |  Test Acc : {test_acc*100:.2f}%')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> All weights are equal\n",
            ">>> Test Loss : 0.413  |  Test Acc : 86.27%\n",
            "CPU times: user 32.1 ms, sys: 1.59 ms, total: 33.7 ms\n",
            "Wall time: 35.9 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2oLVPnebdR2"
      },
      "source": [
        "## Predict New Sentences\n",
        "- The predict_sentiment function is different from previous notebook since **the input sentence has to be at least as long as the largest filter height** used in CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUlSN_AvbeAF",
        "outputId": "365c4ceb-d7bc-4c4f-dd6a-6d8284815970"
      },
      "source": [
        "%%time\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# function for predicting sentiment\n",
        "def predict_class(model, text, nlp=nlp, min_len=max(FILTER_SIZES)):\n",
        "  model.eval()\n",
        "\n",
        "  # preprocess input text\n",
        "  tokenized = [tok.text for tok in nlp.tokenizer(text)]\n",
        "  if len(tokenized) < min_len:\n",
        "    tokenized += [TEXT.pad_token] * (min_len-len(tokenized))  # add padding to adjust input length\n",
        "  indexed = [TEXT.vocab.stoi[tok] for tok in tokenized]\n",
        "  tensor = torch.LongTensor(indexed).unsqueeze(0).to(device)\n",
        "\n",
        "  # predict sentiment with model\n",
        "  preds = model(tensor)\n",
        "  pred_class = preds.argmax(dim=1).item()\n",
        "  return pred_class\n",
        "\n",
        "\n",
        "# predict sentiment\n",
        "for sent in [\"Who is Keyser Söze?\", \"How many minutes are in six hundred and eighteen hours?\", \"What continent is Bulgaria in?\", \"What does WYSIWYG stand for?\"]:\n",
        "  pred_class = predict_class(loaded_model, sent)\n",
        "  print(f'>>> {sent:<55} : {pred_class} - {LABEL.vocab.itos[pred_class]}')\n",
        "\n",
        "print('\\n==================== < Location Tests > ==============================\\n')\n",
        "for sent in [\"Do you know where he decided to build his house?\", \"Do you know where I usually keep my pencils?\", \"Do you know where I usually keep my pencils?\",\n",
        "             \"Do you know where he parked his car?\", \"Do you know where his car is?\", \"Do you know where his car is parked?\", \"Do you know where his car is located?\"]:\n",
        "  pred_class = predict_class(loaded_model, sent)\n",
        "  print(f'>>> {sent:<55} : {pred_class} - {LABEL.vocab.itos[pred_class]}')\n",
        "\n",
        "print('\\n==================== < ABBR Tests > ==================================\\n')\n",
        "for sent in [\"What is the full name of HAM?\", \"What does HAM mean?\", \"Do you know what HAM stand for?\", \n",
        "             \"HAM is short form of what?\", \"Do you know what word is long form of HAM?\", \n",
        "             \"Can you tell me what HAM stand for?\", \"What does HAM stand for?\"]:\n",
        "  pred_class = predict_class(loaded_model, sent)\n",
        "  print(f'>>> {sent:<55} : {pred_class} - {LABEL.vocab.itos[pred_class]}')\n",
        "\n",
        "print('\\n==================== < NUM Tests > ==================================\\n')\n",
        "for sent in [\"Tell me the number of cars.\", 'Could he tell me the number of cars?', \"Could he tell me how many cars there are?\",\n",
        "             \"Would you count the cars?\", \"Will you count the cars?\", \n",
        "             \"Answer me the number of elements in the watch\", \"Answer me how many elements are in the watch\", \"Can you tell me how many elements are in the watch?\"]:\n",
        "  pred_class = predict_class(loaded_model, sent)\n",
        "  print(f'>>> {sent:<55} : {pred_class} - {LABEL.vocab.itos[pred_class]}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 398880/400000 [00:30<00:00, 26821.33it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">>> Who is Keyser Söze?                                     : 0 - HUM\n",
            ">>> How many minutes are in six hundred and eighteen hours? : 3 - NUM\n",
            ">>> What continent is Bulgaria in?                          : 4 - LOC\n",
            ">>> What does WYSIWYG stand for?                            : 5 - ABBR\n",
            "\n",
            "==================== < Location Tests > ==============================\n",
            "\n",
            ">>> Do you know where he decided to build his house?        : 0 - HUM\n",
            ">>> Do you know where I usually keep my pencils?            : 1 - ENTY\n",
            ">>> Do you know where I usually keep my pencils?            : 1 - ENTY\n",
            ">>> Do you know where he parked his car?                    : 0 - HUM\n",
            ">>> Do you know where his car is?                           : 3 - NUM\n",
            ">>> Do you know where his car is parked?                    : 3 - NUM\n",
            ">>> Do you know where his car is located?                   : 4 - LOC\n",
            "\n",
            "==================== < ABBR Tests > ==================================\n",
            "\n",
            ">>> What is the full name of HAM?                           : 1 - ENTY\n",
            ">>> What does HAM mean?                                     : 2 - DESC\n",
            ">>> Do you know what HAM stand for?                         : 5 - ABBR\n",
            ">>> HAM is short form of what?                              : 1 - ENTY\n",
            ">>> Do you know what word is long form of HAM?              : 1 - ENTY\n",
            ">>> Can you tell me what HAM stand for?                     : 1 - ENTY\n",
            ">>> What does HAM stand for?                                : 5 - ABBR\n",
            "\n",
            "==================== < NUM Tests > ==================================\n",
            "\n",
            ">>> Tell me the number of cars.                             : 3 - NUM\n",
            ">>> Could he tell me the number of cars?                    : 3 - NUM\n",
            ">>> Could he tell me how many cars there are?               : 3 - NUM\n",
            ">>> Would you count the cars?                               : 2 - DESC\n",
            ">>> Will you count the cars?                                : 1 - ENTY\n",
            ">>> Answer me the number of elements in the watch           : 3 - NUM\n",
            ">>> Answer me how many elements are in the watch            : 3 - NUM\n",
            ">>> Can you tell me how many elements are in the watch?     : 1 - ENTY\n",
            "CPU times: user 489 ms, sys: 75.7 ms, total: 565 ms\n",
            "Wall time: 804 ms\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}