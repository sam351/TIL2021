{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04-2_Bag-of-Words",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVMvcJDfk1RV",
        "outputId": "c6b25183-04ad-47bd-b45f-09075ec3c8a8"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "!pip install konlpy\n",
        "import konlpy\n",
        "from konlpy.tag import Okt, Kkma, Mecab\n",
        "\n",
        "import re\n",
        "\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "print(f'\\n\\n>>> nltk : {nltk.__version__}  |  konlpy : {konlpy.__version__}  |  sklearn : {sklearn.__version__}')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.2MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a5/9781e2ef4ca92d09912c4794642c1653aea7607f473e156cf4d423a881a1/JPype1-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 40.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: colorama, beautifulsoup4, JPype1, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n",
            "\n",
            "\n",
            ">>> nltk : 3.2.5  |  konlpy : 0.5.2  |  sklearn : 0.22.2.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtSGCCvyxfv0"
      },
      "source": [
        "## 한글 Bag of words\n",
        "- Bag of Words : 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법\n",
        "- BoW를 만드는 과정<br>\n",
        "  1) 각 단어에 고유한 정수 인덱스 부여\n",
        "  2) 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSR4L66Txf03",
        "outputId": "45d54895-9a81-4902-ae09-ff139d95c4fb"
      },
      "source": [
        "sent1 = \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\"\n",
        "sent2 = \"소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.\"\n",
        "\n",
        "# 전처리 - 정규 표현식을 통해 온점 제거\n",
        "sent1 = re.sub(\"(\\.)\", \"\", sent1)\n",
        "sent2 = re.sub(\"(\\.)\", \"\", sent2)\n",
        "\n",
        "# OKT 형태소 분석기를 통해 토큰화 수행\n",
        "okt = Okt()\n",
        "tokens1 = okt.morphs(sent1)\n",
        "tokens2 = okt.morphs(sent2)\n",
        "\n",
        "# 토큰별 index 생성 및 빈도 계산\n",
        "def get_bow(tokens):\n",
        "  word2index = {}\n",
        "  bow = []\n",
        "  for tk in tokens:\n",
        "          if tk not in word2index.keys():\n",
        "              word2index[tk] = len(word2index)\n",
        "              bow.insert(len(word2index)-1,1)\n",
        "          else:\n",
        "              index = word2index.get(tk)\n",
        "              bow[index]=bow[index]+1\n",
        "  return word2index, bow\n",
        "\n",
        "\n",
        "for tmp_tokens in [tokens1, tokens2]:\n",
        "  tmp_dict, tmp_list = get_bow(tmp_tokens)\n",
        "  print('word2index :', tmp_dict)\n",
        "  print('bow :', tmp_list)\n",
        "  print()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word2index : {'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n",
            "bow : [1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n",
            "\n",
            "word2index : {'소비자': 0, '는': 1, '주로': 2, '소비': 3, '하는': 4, '상품': 5, '을': 6, '기준': 7, '으로': 8, '물가상승률': 9, '느낀다': 10}\n",
            "bow : [1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfF_A0vaLt4u"
      },
      "source": [
        "## 영어 Bag of words (CountVectorizer)\n",
        "- 사이킷런의 CountVectorizer는 띄어쓰기만을 기준으로 단어를 자르는 낮은 수준의 토큰화 지원\n",
        "- tokenizer 특성상 한글에는 적용 무리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJ7UcZ-_Zx5m",
        "outputId": "ed8c8fde-8cd8-4be6-e997-1eeba3874f38"
      },
      "source": [
        "text = ['you know I want your love. because I love you.']\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "bow_arr = vectorizer.fit_transform(text)\n",
        "print(bow_arr.toarray())  # convert CSR to array\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 2 1 2 1]]\n",
            "{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjFBE94Sb8c0",
        "outputId": "66871f38-1135-4d2c-f12e-905d8aa3cdcd"
      },
      "source": [
        "# build Bag of words with custom stopwords list\n",
        "text = [\"Family is not an important thing. It's everything.\"]\n",
        "vectorizer = CountVectorizer(stop_words=['the', 'a', 'an', 'is', 'not'])\n",
        "\n",
        "bow_arr = vectorizer.fit_transform(text)\n",
        "print(bow_arr.toarray())  # convert CSR to array\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 1 1 1]]\n",
            "{'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I16O5Sv1d5u6",
        "outputId": "03c07b31-ad97-40c4-a5cc-f8c9b354bdf7"
      },
      "source": [
        "# build Bag of words with sklearn stopwords list\n",
        "text = [\"Family is not an important thing. It's everything.\"]\n",
        "vectorizer = CountVectorizer(stop_words='english')  # sklearn Doc said \"There are several known issues with 'english' and you should consider an alternative\"\n",
        "\n",
        "bow_arr = vectorizer.fit_transform(text)\n",
        "print(bow_arr.toarray())  # convert CSR to array\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 1 1]]\n",
            "{'family': 0, 'important': 1, 'thing': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IJxrH_UeAoL",
        "outputId": "9ded920f-e6b7-48f5-f7b7-72829cb357f9"
      },
      "source": [
        "# build Bag of words with nltk stopwords list\n",
        "text = [\"Family is not an important thing. It's everything.\"]\n",
        "sw = stopwords.words('english')\n",
        "print(len(sw))  # stopwords vocab size\n",
        "vectorizer = CountVectorizer(stop_words=sw)\n",
        "\n",
        "bow_arr = vectorizer.fit_transform(text)\n",
        "print(bow_arr.toarray())  # convert CSR to array\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179\n",
            "[[1 1 1 1]]\n",
            "{'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UXM0f9XcUZR"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}